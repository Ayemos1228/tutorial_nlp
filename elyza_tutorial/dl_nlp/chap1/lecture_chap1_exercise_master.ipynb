{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1回講義 演習 PyTorchの基礎を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本演習では、以下の構成に従って、PyTorchの基礎を学んでいきます。\n",
    "\n",
    "1. PyTorchとは\n",
    "    - 1.1 PyTorchとは\n",
    "    - 1.2 ライブラリの構成\n",
    "2. Tensor\n",
    "    - 2.1　基本的な行列\n",
    "    - 2.2 型の定義\n",
    "    - 2.3 サイズの確認\n",
    "    - 2.4 変形\n",
    "    - 2.5 演算\n",
    "    - 2.6 Numpy, list, scalarへの変換\n",
    "    - 2.7 デバイス(CPU/CUDA)の指定\n",
    "3. Autograd\n",
    "4. nn\n",
    "    - 4.1 Module\n",
    "    - 4.2 活性化関数\n",
    "    - 4.3 誤差関数\n",
    "    - 4.4 モジュール化\n",
    "    - 4.5 最適化\n",
    "    - 4.6 学習\n",
    "    - 4.7 モデルの保存・読み込み・再学習\n",
    "5. Tips:torchvision & DataLoader\n",
    "5. 課題：PyTorchを使ってMLPを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.chdir('/root/userspace/chap1/') # 自分のファイルがあるパスにしてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorchとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. PyTorchとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchは, 主にFacebookによって開発されているニューラルネットワーク用のライブラリです.\n",
    "TensorFlowやTheanoでは, ネットワークの計算グラフを定義した後にコンパイルしてからデータを流す (静的フレームワーク) のに対し, PyTorchではデータが流れるごとに計算グラフを動的に構築する (動的フレームワーク) ため, データごとにグラフの形状が異なる自然言語処理などに向いているとされています."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式ドキュメント: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ライブラリの構成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/pytorch_overview.png\" width=\"600mm\">\n",
    "\n",
    "出典: PyTorch公式 (https://pytorch.org/about/)\n",
    "\n",
    "出典: PyTorchのススメ (https://www.slideshare.net/yuyasoneoka/pytorch-80883065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorは、変数をグラフ上のノードとして表すためのクラスです。\n",
    "\n",
    "NumPyのndarrayに似たAPIのまま、GPU上での高速計算（CUDA）を扱えるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1 基本的な行列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "基本的な行列は、numpyと同様の関数を使って作ることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 全要素が1の行列 (numpy.ones)\n",
    "a = torch.ones((2, 3))\n",
    "print(\"# torch.ones:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 全要素が0の行列 (numpy.ones)\n",
    "a = torch.zeros((2, 3))\n",
    "print(\"# torch.zeros:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 指定した値で満たされた行列 (numpy.full)\n",
    "a = torch.full((2, 3), fill_value=99)\n",
    "print(\"# torch.full:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 単位行列 (numpy.eye)\n",
    "a = torch.eye(2)\n",
    "print(\"# torch.eye:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "一般的な乱数は大体揃っています. シードは`torch.manual_seed`で指定できます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "# 標準正規分布 (numpy.random.randn)\n",
    "print(\"# torch.randn（正規分布）:\")\n",
    "print(torch.randn(2))\n",
    "print()\n",
    "\n",
    "# [0, 1)の一様分布 (numpy.random.rand)\n",
    "print(\"# torch.rand（一様分布）:\")\n",
    "print(torch.rand(2))\n",
    "print()\n",
    "\n",
    "# ベルヌーイ分布\n",
    "probs = torch.rand((2, 3))\n",
    "print(\"# 確率p:\")\n",
    "print(probs)\n",
    "print()\n",
    "print(\"# torch.bernoulli（ベルヌーイ分布）:\")\n",
    "print(torch.bernoulli(probs))\n",
    "print()\n",
    "\n",
    "# 多項分布 (np.random.multinomial)\n",
    "probs = torch.tensor([0.2, 0.4, 0.4])\n",
    "print(\"# torch.multinominal（多項分布）:\")\n",
    "print(torch.multinomial(probs, num_samples=10, replacement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.2 型の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "dtypeの引数を指定することで、Tensorの型を定義することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.float)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3), dtype=torch.int)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3), dtype=torch.long)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.3 サイズの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`.size()` を用いることで、Tensorのサイズを確認することができます。\n",
    "\n",
    "(`.size()`のエイリアスとして`.shape`も存在するので、numpyのように`.shape`で取得することも可能です。)\n",
    "\n",
    "\n",
    "引数を指定することで特定の次元のサイズのみを取得することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3))\n",
    "\n",
    "print(a.size())  # Tensorのサイズを取得\n",
    "print(a.shape)  # .shapeでも可能\n",
    "\n",
    "print(a.size(0))  # Tensorの0次元目のサイズを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 変形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの変形も同様に行えます。\n",
    "\n",
    "軸方向を指定する引数名はnumpyではaxisでしたが、PyTorchではdimであることに注意する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.1 次元の追加・除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6)  # 等差数列を作成 (numpy.arange)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "b = a.reshape(2, 3)  # 変形 (numpy.reshape)\n",
    "print(\"# reshape(2,3):\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.view(1, 2, 3, 1) # dimの追加も可能\n",
    "print(\"# view(1,2,3,1):\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# a.viewでエラーが出ることがある場合は.contiguous()を呼んでからview()する (参照: https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930)\n",
    "print(a.contiguous().view(1, 2, 3, 1))\n",
    "print()\n",
    "\n",
    "b = a.view(-1, 2) # 最後のdimのsizeを2に, 他のdimは\"潰す\"\n",
    "print(\"# view(-1,2)で最後のdimのsizeを2に, 他のdimは潰す:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.unsqueeze(dim=1)  # 新しいdimを追加する。 a[:, None]でもOK (numpy.expand_dims)\n",
    "print(\"# unsqueeze(dim=1)で次元をdim1に追加:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "c = b.squeeze()  # squeezeを用いるとsizeが1の次元が除去される (numpy.squeeze)\n",
    "print(\"# squeeze()でサイズ1の次元を削除:\")\n",
    "print(c)\n",
    "print(c.size())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.2 次元の入れ替え・変形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(1, 2, 3)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "b = a.transpose(0, 2)  # dim0とdim2の次元を入れ替える (numpy.transpose)\n",
    "print(\"# transpose(0, 2)でdim0とdim2の次元を入れ替える:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.permute(0, 2, 1)  # 指定したdimの順に次元を入れ替える\n",
    "print(\"# permute(0, 2, 1)で順番を並び替える:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.3 分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`torch.split`は指定したサイズでテンソルの分割を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(10).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "b = torch.split(a, 2, dim=1)  # 2要素ごとに分割(割り切れない場合は最後の要素が余りになる) (numpy.split)\n",
    "print(\"# torch.splitで2個ずつに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()\n",
    "\n",
    "b = torch.split(a, [1, 3, 1], dim=1)  # リストを指定するとそのサイズで分割\n",
    "print(\"# torch.splitで1,3,1個ずつに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`torch.chunk`は指定した個数にテンソルを分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(10).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "b = torch.chunk(a, 5, dim=1) # 5グループに分割\n",
    "print(\"# chunkで5グループに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()\n",
    "\n",
    "b = torch.chunk(a, 3, dim=1) # 3グループに分割(割り切れない場合は最後の要素が小さくなる)\n",
    "print(\"# chunkで3グループに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 連結"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cat`は既存のdimに沿ってテンソルを連結します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(8).reshape(2, 4)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "c = torch.cat([a, b], dim=1) # 既存のdimで連結する. 他のdimのsizeは揃っている必要がある (numpy.concatenate)\n",
    "print(\"# torch.catでdim1方向に連結:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.stack`は新しいdimを作成し、そのdimに沿ってテンソルを連結します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "c = torch.stack([a, b], dim=2) # 新しいdimで連結する. 既存のdimのsizeはすべて揃っている必要がある (numpy.stack)\n",
    "print(\"# torch.stackで新しい次元（dim2）方向に連結:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 集約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.gather`は指定したdimに沿って対象の行列(input)のindexの要素を集約し、indexと同じサイズの行列を出力します。\n",
    "\n",
    "indexはinputと同じ次元数である必要があります。\n",
    "\n",
    "2次元の行列(input)に対する出力(out)は以下のようになります。\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0のとき\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1のとき\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# dim=0の場合、第0軸方向をindexに沿って集約、その他の軸方向については順番に集約する\n",
    "index = torch.tensor([[1, 0, 1]])\n",
    "print(index.size())\n",
    "# この場合、第0軸方向は[1, 0, 1]、第1軸方向は[0, 1, 2]と集約されるので、[[a[1, 0], a[0, 1], a[1, 2]]]が出力される\n",
    "b = torch.gather(a, dim=0, index=index)\n",
    "print(\"# torch.gatherでdim0方向に関しては[0,1,0]で集める:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# dim=1の場合、第1軸方向をindexに沿って集約、その他の軸方向については順番に集約する\n",
    "index = torch.tensor([[1, 0], [0, 1]])\n",
    "print(index.size())\n",
    "# この場合、第0軸方向は[[0, 0], [1, 1]]、第1軸方向は[[1, 0], [0, 1]]と集約されるので、[[a[0, 1], a[0, 0]], [a[1, 0], a[1, 1]]が出力される\n",
    "b = torch.gather(a, dim=1, index=index)\n",
    "print(\"# torch.gatherでdim1方向に関しては[0,1][1,0]で集める:\")\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.index_select`も指定したdimに沿って対象の行列(input)のindexの要素を集約しますが、numpyのindexingに似た挙動です。\n",
    "\n",
    "indexはinputと同じ次元数である必要はなく、出力もindexと異なるサイズの出力となりえます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# dim=0\n",
    "index = torch.tensor([1, 0, 1])\n",
    "print(index.size())\n",
    "b = torch.index_select(a, dim=0, index=index)  # a[[1, 0, 1], :]\n",
    "print(\"# torch.index_selectでdim0方向に関して[1,0,1]で要素（行）を集める:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# dim=1\n",
    "index = torch.tensor([0, 1, 2, 1, 0])\n",
    "print(index.size())\n",
    "b = torch.index_select(a, dim=1, index=index)  # [:, [0, 1, 2, 1, 0]]\n",
    "print(\"# torch.index_selectでdim1方向に関して[0,1,2,1,0]で要素（列）を集める:\")\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.5 演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.1 スカラー演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpyと同様にスカラー演算を実装することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.rand((2, 3))\n",
    "b = torch.rand((2, 3))\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "print(\"# 足し算:\")\n",
    "print(a + b)\n",
    "print()\n",
    "\n",
    "print(\"# 引き算:\")\n",
    "print(a - b)\n",
    "print()\n",
    "\n",
    "print(\"# 掛け算:\")\n",
    "print(a * b)\n",
    "print()\n",
    "\n",
    "print(\"# 割り算:\")\n",
    "print(a / b)\n",
    "print()\n",
    "\n",
    "print(\"# log:\")\n",
    "print(torch.log(a))\n",
    "print()\n",
    "\n",
    "print(\"# exp:\")\n",
    "print(torch.exp(a))\n",
    "print()\n",
    "\n",
    "print(\"# ルート:\")\n",
    "print(torch.sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.2 集約演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "集約演算もnumpyと同様に実装できますが、集約する次元をaxisではなくdimで指定することに注意する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(10, dtype=torch.float32).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# 合計(dim0):\")\n",
    "print(torch.sum(a, dim=0)) # a.sum(0)でも可\n",
    "print()\n",
    "\n",
    "print(\"# 平均(dim1):\")\n",
    "print(torch.mean(a, dim=1))\n",
    "print()\n",
    "\n",
    "print(\"# 分散(全体):\")\n",
    "print(torch.var(a)) # dimを指定しない場合は全体に対して適用\n",
    "print()\n",
    "\n",
    "print(\"# 標準偏差(dim -1, 最後の次元):\")\n",
    "print(torch.std(a, dim=-1)) # dim=-1とすると最後の次元に対して適用\n",
    "print()\n",
    "\n",
    "print(\"# 最大値、argmax(dim0):\")\n",
    "print(torch.max(a, dim=0)) # torch.maxは, maxとargmaxの両方を返す(torch.minも同様)\n",
    "print()\n",
    "\n",
    "print(\"# 最大値:\")\n",
    "print(torch.max(a)) # dimを指定しない場合はmax(or min)のみ返す\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.3 $Lp$ノルム\n",
    "\n",
    "行列のノルムも、numpy同様に求めることができます。(numpy.linalg.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "    ||x||_p = \\left(x^p_1 + x^p_2 + \\ldots + x^p_N\\right)^{1/p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(3, dtype=torch.float)\n",
    "print(a)\n",
    "print(torch.norm(a, p=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.4 行列・テンソル積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "行列の積もnumpy同様にdot, matmulで実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(4)\n",
    "b = torch.ones(4)\n",
    "\n",
    "c = torch.dot(a, b)\n",
    "print(\"# torch.dot（ベクトルの内積）:\")\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "b = torch.ones((3, 4))\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "print(\"# torch.matmul（行列積）:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3階のテンソルの内積計算には`torch.bmm(batch1, batch2)`が使えます。\n",
    "\n",
    "batch1とbatch2は3階のテンソルで、それぞれが(b, n, m)と(b, m, p)のsizeを有する場合に(b, n, p)のテンソルを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3, 4)\n",
    "b = torch.ones(2, 4, 5)\n",
    "\n",
    "print(\"# torch.bmm（batch matrix matrix product）:\")\n",
    "c = torch.bmm(a, b)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "3階以上のテンソルを含む計算には`np.einsum`と同様にアインシュタインの縮約記法を用いる`einsum`での実装が便利です。\n",
    "\n",
    "einsumを用いると、テンソル積の計算を行う行列のサイズを添字で表現し、計算を行う前と後のサイズを与えて計算を行うことができます。\n",
    "\n",
    "詳しくはドキュメント（ https://pytorch.org/docs/stable/torch.html#torch.einsum ）を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3, 4))\n",
    "b = torch.ones((2, 3))\n",
    "\n",
    "c = torch.einsum('ijk,ij->k', (a, b))\n",
    "print(c)\n",
    "\n",
    "sum_c = torch.einsum('ijk,ij->', (a, b))\n",
    "print(sum_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.5.5 条件演算子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "時系列問題(RNNなど)でmaskを作るときなどに有効です.\n",
    "```\n",
    "torch.where(条件式, Trueの場合, Falseの場合)\n",
    "```\n",
    "のように書きます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "a = torch.randn((2, 3))\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# torch.whereで0以下のmask:\")\n",
    "masked_a = torch.where(a > 0, torch.ones_like(a), torch.zeros_like(a))\n",
    "print(masked_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpy.clipのように、値を一定の範囲でクリッピングします. logなどでのアンダーフローを防ぐときなどに有効です."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "x_clipped = torch.clamp(x, 1e-10, 1e+10)\n",
    "print(\"# torch.clampでクリッピング:\")\n",
    "print(x_clipped)\n",
    "print(torch.log(x_clipped))\n",
    "print()\n",
    "\n",
    "print(\"# クリッピングしないとnanがでる:\")\n",
    "print(torch.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.5.6 比較演算子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.ge, torch.gt, torch.le, torch.lt, torch.eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "値の大小の比較を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# a >= 3:\")\n",
    "print(torch.ge(a, 3))  # greater than or equal to\n",
    "print(a >= 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a > 3:\")\n",
    "print(torch.gt(a, 3))  # greater than\n",
    "print(a > 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a <= 3:\")\n",
    "print(torch.le(a, 3))  # less than or equal to\n",
    "print(a <= 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a < 3:\")\n",
    "print(torch.lt(a, 3))  # less than\n",
    "print(a < 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a == 3:\")\n",
    "print(torch.eq(a, 3))  # equal to\n",
    "print(a == 3)  # pythonの比較演算子を用いることも可能\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.6 Numpy, list, scalarへの変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpy, list, scalarへの変換は以下のようにして行います."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3))\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# numpy.ndarrayへの変換\n",
    "print(\"# a.numpy():\")\n",
    "print(a.numpy())\n",
    "print()\n",
    "\n",
    "# listへの変換\n",
    "print(\"# a.tolist():\")\n",
    "print(a.tolist())\n",
    "print()\n",
    "\n",
    "# scalarへの変換には.item()を使います\n",
    "print(\"# a.item():\")\n",
    "print(a.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 デバイス(CPU/CUDA)の指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorを扱うデバイスを指定するには、`device`引数や`.to()`メソッドなどを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# GPUへの移動(すべて同じです)\n",
    "b = a.cuda()\n",
    "print(b)\n",
    "b = a.to('cuda')\n",
    "print(b)\n",
    "b = torch.ones(1, device='cuda')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# CPUへの移動(すべて同じです)\n",
    "c = b.cpu()\n",
    "print(c)\n",
    "c = b.to('cpu')  \n",
    "print(c)\n",
    "c = torch.ones(1, device='cpu')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDAが使えるかどうかは`torch.cuda.is_available()`でわかるので、あらかじめこれでdeviceを取得しておくと便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.ones(1, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PyTorchでは、Tensorの自動微分をサポートするautogradの機能が提供されております。\n",
    "\n",
    "autogradの仕組みを理解する上で`Tensor`に加えて重要なクラスとして`Function`があります。\n",
    "\n",
    "FunctionはTensorを入力としてTensorを出力する関数であり、Tensorをノード、Functionをエッジとして計算グラフが構築されます。\n",
    "\n",
    "各Tensorは`.grad_fn`という属性を有しており、これはそのTensorを作成したFunctionを参照しています。（ユーザが自分で作成したTensorのgrad_fnはNoneになります）\n",
    "<img src=\"figures/tensor.png\" width=\"400mm\">\n",
    "\n",
    "\n",
    "autogradを用いるには、まず、計算グラフを構築した後、forward関数によって入力のTensorから出力のTensorに対する順伝播の計算を行います。\n",
    "\n",
    "その後、backward関数を呼ぶことにより、`requires_grad=True`を指定したすべてのTensorの目的関数に関する勾配が計算されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 順伝播の計算\n",
    "x = torch.randn(4, 4)\n",
    "y = torch.randn(4, 1)\n",
    "\n",
    "w = torch.randn(4, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "\n",
    "# 目的関数の定義\n",
    "loss = (y_pred - y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ユーザが作成したTensorはgrad_fn=None\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "print(w.grad_fn)\n",
    "print(b.grad_fn)\n",
    "print()\n",
    "\n",
    "# Functionによって計算されたTensorはgrad_fnを有する\n",
    "print(y_pred.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# まだ勾配は計算されていない\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 逆伝播\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# requires_grad=Trueを指定した変数は勾配が計算されている\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`.detach()`を使うことにより、Tensorの勾配計算を行わないようにすることもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = torch.randn(4, 1)\n",
    "\n",
    "w = torch.randn(4, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "b = b.detach()  # bの勾配計算を停止\n",
    "\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)  # 勾配を有する\n",
    "print(b.grad)  # 勾配を有さない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "また、`with torch.no_grad():`でくくることで、その下で定義したTensorの勾配計算をまとめて停止させることが可能です。\n",
    "\n",
    "これは、学習済みのモデルを評価する際に、モデルが`requires_grad=True`となっているパラメータを有する場合でも勾配計算を行わないようにしたいときなどに有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = torch.matmul(x, w) + b  # y_predと同様の計算を行う\n",
    "\n",
    "print('requires_grad of y_pred:', y_pred.requires_grad)  # requires_grad=True\n",
    "print('requires_grad of y_eval:', y_eval.requires_grad)  # requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "autogradは自動微分を可能にする強力な機能ですが、巨大なニューラルネットワークを低レベルのautogradのみで実装するのは大変です。\n",
    "\n",
    "ニューラルネットワークを構築する際には、学習可能なパラメータを有するいくつかのレイヤーを定義することが一般的ですが、Pytorchにはこれを行う上で`nn`という便利な高レベルのパッケージが存在します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1 Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`nn`には、ニューラルネットワークにおけるレイヤーのような役目を果たす`Module`が実装されています。\n",
    "\n",
    "Tensorを入力としてTensorを出力しますが、学習可能なパラメータなどの内部状態を有し、forward関数とbackward関数を有します。\n",
    "\n",
    "線形層や畳み込み層など、一般的なレイヤーは一通り実装されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# 線形層\n",
    "nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# 畳み込み層\n",
    "nn.Conv1d(input_dim, output_dim, kernel_size)\n",
    "\n",
    "# LSTM\n",
    "nn.LSTM(input_dim, hidden_dim, num_layers)\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.2 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "torch.nn.functionalに一般的な活性化関数は一通り揃っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(34)\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(torch.sigmoid(x)) # F.sigmoidでもよいが、deprecatedなのでこちらが推奨されている\n",
    "print()\n",
    "\n",
    "print(F.relu(x))\n",
    "print()\n",
    "\n",
    "print(torch.tanh(x)) # F.tanhでもよいが、deprecatedなのでこちらが推奨されている\n",
    "print()\n",
    "\n",
    "print(F.leaky_relu(x, 0.2))\n",
    "print()\n",
    "\n",
    "print(F.softplus(x))\n",
    "print()\n",
    "\n",
    "print(F.softmax(x, dim=-1)) # 正規化したいdimを指定する. 最後のdimに対して行いたいときはdim=-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.3 誤差関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "一般的な誤差関数も一通り揃っています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# L1誤差\n",
    "nn.L1Loss()\n",
    "\n",
    "# 平均二乗誤差\n",
    "nn.MSELoss()\n",
    "\n",
    "# 交差エントロピー誤差\n",
    "nn.CrossEntropyLoss()\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.4 モジュール化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "自分で層・ネットワークなどをクラス化したい場合は, `nn.Module`のサブクラスとして作成し、`__init__`と`forward`をoverrideすることで実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):  # nn.Moduleを継承する\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):  # __init__をoverride\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):  # forwardをoverride\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(2, 3, 1)\n",
    "print(mlp)\n",
    "print()\n",
    "\n",
    "x = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = mlp(x) # forward(x)が呼ばれる\n",
    "print(\"# feedforward：\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "print(\"# mlp.parameters()でモデルのパラメータ取得：\")\n",
    "print(mlp.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.5 最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "torch.optimに一般的なoptimizerが実装されています。\n",
    "\n",
    "勾配のリセットは`.zero_grad()`で、パラメータの更新は`.step()`で行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# optimizerの定義\n",
    "optimizer = optim.SGD([W1, W2], lr=0.1)\n",
    "\n",
    "# 勾配のリセット\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# パラメータの更新\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.6 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "これまで見てきたモジュールなどを用いてMLPのモデルを学習させる一連の流れを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# XORをMLPで行う\n",
    "x = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "t = torch.Tensor([0, 1, 1, 0])\n",
    "\n",
    "# モデルの定義\n",
    "mlp = MLP(2, 3, 1)\n",
    "\n",
    "# 誤差関数の定義\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "# 最適化の定義\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.1)  # Moduleのパラメータは.parameters()で取得できる\n",
    "\n",
    "# モデルを訓練モードにする（Dropout等に関係）\n",
    "mlp.train()\n",
    "\n",
    "for i in range(1000):\n",
    "    # 順伝播\n",
    "    y_pred = mlp(x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    loss = criterion(y_pred, t.unsqueeze(1))\n",
    "    \n",
    "    # 逆伝播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの更新\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.7 モデルの保存・読み込み・再学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "モデルを保存する際には、`torch.save()`を用いますが、モデルのインスタンスを直接保存するのではなく、モデルのパラメータの情報を有するstate_dictを保存し、読み込む際にもstate_dictを読み込んでモデルのインスタンスにloadするのが一般的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(list(mlp.parameters()))\n",
    "print()\n",
    "\n",
    "# state_dictの取得\n",
    "state_dict = mlp.state_dict()\n",
    "print(state_dict)\n",
    "\n",
    "# モデルの保存\n",
    "torch.save(state_dict, './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "mlp2 = MLP(2, 3, 1)\n",
    "print(list(mlp2.parameters()))  # ランダムな初期値\n",
    "print()\n",
    "\n",
    "# 学習済みパラメータの読み込み\n",
    "state_dict = torch.load('./model.pth')\n",
    "mlp2.load_state_dict(state_dict)\n",
    "print(list(mlp2.parameters()))  # 学習済みパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tips: torchvision & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvisionは画像認識のためのデータセットや前処理、学習済みモデルなどが収められたpytorch公式のライブラリです。\n",
    "\n",
    "MNISTなどのよく使われるデータセットを`torchvision.datasets`から簡単に読み込めるほか、前処理は`transform`としてまとめて`torch.utils.data.DataLoader`に渡すだけでOKです。\n",
    "\n",
    "MNISTを使用する際の例を以下に示します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(in_dim))\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data/mnist', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 課題：PyTorchを使ってMLPを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.log_softmax(self.linear2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim  = 784\n",
    "hid_dim = 200\n",
    "out_dim = 10\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "\n",
    "mlp = MLP(in_dim, hid_dim, out_dim).to(device)\n",
    "\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()  # Negative Log Liklihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(in_dim))\n",
    "])\n",
    "\n",
    "# torchvisionのdatasetsを使ってMNISTのデータを取得\n",
    "# ミニバッチ化や前処理などの処理を行ってくれるDataLoaderを定義\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=False, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    preds_train = []\n",
    "    preds_valid = []\n",
    "    trues_train = []\n",
    "    trues_valid = []\n",
    "    \n",
    "    mlp.train()\n",
    "    for x, t in dataloader_train:\n",
    "        true = t.tolist()\n",
    "        trues_train.extend(true)\n",
    "\n",
    "        # 勾配の初期化\n",
    "        mlp.zero_grad()\n",
    "        \n",
    "        # テンソルをGPUに移動\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # 順伝播\n",
    "        y = mlp.forward(x)\n",
    "        \n",
    "        # 誤差の計算\n",
    "        loss = criterion(y, t)\n",
    "        \n",
    "        # 誤差の逆伝播\n",
    "        loss.backward()\n",
    "        \n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        # モデルの出力を予測値のスカラーに変換\n",
    "        pred = y.argmax(1).tolist()\n",
    "        preds_train.extend(pred)\n",
    "        \n",
    "        losses_train.append(loss.tolist())\n",
    "    \n",
    "    mlp.eval()\n",
    "    for x, t in dataloader_valid:\n",
    "        true = t.tolist()\n",
    "        trues_valid.extend(true)\n",
    "\n",
    "        # テンソルをGPUに移動\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # 順伝播\n",
    "        y = mlp.forward(x)\n",
    "\n",
    "        # 誤差の計算\n",
    "        loss = criterion(y, t)\n",
    "        \n",
    "        # モデルの出力を予測値のスカラーに変換\n",
    "        pred = y.argmax(1).tolist()\n",
    "        preds_valid.extend(pred)\n",
    "        \n",
    "        losses_valid.append(loss.tolist())\n",
    "        \n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, F1: {:.3f}], Valid [Loss: {:.3f}, F1: {:.3f}]'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        f1_score(trues_train, preds_train, average='macro'),\n",
    "        np.mean(losses_valid),\n",
    "        f1_score(trues_valid, preds_valid, average='macro')\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
