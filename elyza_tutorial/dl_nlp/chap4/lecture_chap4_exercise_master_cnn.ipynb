{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回 演習（１）Text CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNは一般的には画像認識の分野で広く使われていますが、自然言語処理のタスクにおいても良い効果を示すことが知られています。\n",
    "\n",
    "具体的には、RNNに比べて計算の並列化がしやすいために高速であり、時間的に離れた情報間の関係も学習しやすいなどの性質を持ちます。\n",
    "\n",
    "CNNの入力は、画像認識のタスクにおいては画像のピクセルからなる行列ですが、自然言語処理のタスクにおいては入力は行列で表現された文書や文章になります。\n",
    "\n",
    "行列の各行は一つのトークン（単語や文字）に対応する特徴ベクトルとなっており、任意のフィルタサイズのカーネルで、行全体をまとめてスライドするように1次元の畳み込みを行います。\n",
    "\n",
    "\n",
    "今回はText CNNというモデルを使い、CNNで文章のクラス分類を行います。\n",
    "\n",
    "\n",
    "原論文：[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Text_CNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引用元：https://arxiv.org/abs/1510.03820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from nltk import Tree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError:  # iLect環境\n",
    "    import os\n",
    "    os.chdir('/root/userspace/chap4/')\n",
    "    from utils import Vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊なトークンを定義しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、評判分析データセットの1つであるSST（Stanford Sentiment Treebank）と呼ばれるデータセットを用います。\n",
    "\n",
    "映画レビューサイトのレビューが構文木解析され、文のフレーズごとにポジティブ・ニュートラル・ネガティブのラベルが振られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの中身は以下のような構文木になっており、そのままではベクトルに変換できないので、nltkライブラリのTreeを用いてパースします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))\n",
      "\n",
      "['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n"
     ]
    }
   ],
   "source": [
    "with open('./data/trees/train.txt') as f:\n",
    "    line = f.readline()\n",
    "print(line)\n",
    "\n",
    "tree = Tree.fromstring(line)\n",
    "print(tree.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、各文章には\n",
    "\n",
    "0: very negative  \n",
    "\n",
    "1: negative\n",
    "\n",
    "2: neutral\n",
    "\n",
    "3: positive\n",
    "\n",
    "4: very positive\n",
    "\n",
    "という5つのラベルが振られていますが、今回はタスクを単純にするために\n",
    "\n",
    "0: negative\n",
    "\n",
    "1: positive\n",
    "\n",
    "のラベルのみを振り直し、2クラス分類として定式化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "LABEL_DICT = {\n",
    "    '0': 0,  # very negative -> negative \n",
    "    '1': 0,  # negative\n",
    "    '2': None,  # neutral -> 今回は使わない\n",
    "    '3': 1,  # positive\n",
    "    '4': 1,  # ver positive -> positive\n",
    "}\n",
    "\n",
    "original_label = tree.label()\n",
    "print(original_label)\n",
    "\n",
    "new_label = LABEL_DICT[original_label]\n",
    "print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            tree = Tree.fromstring(line)  # パース\n",
    "            x = [word.lower() for word in tree.leaves()]  # ex: ['i', 'have' 'a', 'pen']\n",
    "            label = LABEL_DICT[tree.label()]  # {0, 1, None}\n",
    "            if label is None:  # neutralラベルは除外する\n",
    "                continue\n",
    "            X.append(x)\n",
    "            y.append(label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_X, train_Y = load_data('./data/trees/train.txt')\n",
    "valid_X, valid_Y = load_data('./data/trees/dev.txt')\n",
    "test_X, test_Y = load_data('./data/trees/test.txt')\n",
    "# 演習用にデータサイズを縮小\n",
    "train_X = train_X[:len(train_X)//2]\n",
    "train_Y = train_Y[:len(train_Y)//2]\n",
    "valid_X = train_X[:len(valid_X)//2]\n",
    "valid_Y = train_Y[:len(valid_Y)//2]\n",
    "test_X = train_X[:len(test_X)//2]\n",
    "test_Y = train_Y[:len(test_Y)//2]\n",
    "# ラベルを確認\n",
    "labels = set(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 語彙の作成\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sentence):\n",
    "    \"\"\"\n",
    "    単語のリストをIDのリストに変換する\n",
    "    :param vocab: Vocabのインスタンス\n",
    "    :param sentence: list of str\n",
    "    :return indices: list of int\n",
    "    \"\"\"\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = [sentence_to_ids(vocab, x) for x in train_X]\n",
    "valid_X = [sentence_to_ids(vocab, x) for x in valid_X]\n",
    "test_X = [sentence_to_ids(vocab, x) for x in test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(self, X, Y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        :param X: list, 入力言語の文章（単語IDのリスト）のリスト\n",
    "        :param Y: list, 出力言語の文章（単語IDのリスト）のリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data = list(zip(X, Y))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:  # サンプルの順番をシャッフルする\n",
    "            self.data = shuffle(self.data, random_state=random_state)\n",
    "        self.start_index = 0  # ポインタの位置を初期化する\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "\n",
    "        # バッチを取得\n",
    "        X, Y = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
    "        # 短い系列の末尾をパディングする\n",
    "        lengths_X = [len(s) for s in X]\n",
    "        max_length_X = max(lengths_X)\n",
    "        padded_X = [self.pad_seq(s, max_length_X) for s in X]\n",
    "        # tensorに変換\n",
    "        batch_X = torch.tensor(padded_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "\n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "\n",
    "        return batch_X, batch_Y\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_seq(seq, max_length):\n",
    "        \"\"\"\n",
    "        系列の末尾をパディングする\n",
    "        :param seq: list of int, 単語のインデックスのリスト\n",
    "        :param max_length: int, バッチ内の系列の最大長\n",
    "        :return seq: list of int, 単語のインデックスのリスト\n",
    "        \"\"\"\n",
    "        seq += [PAD for i in range(max_length - len(seq))]\n",
    "        return seq    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, class_num, kernel_num, kernel_sizes, dropout, static):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力言語の語彙数\n",
    "        :param embedding_size: int, 埋め込みベクトルの次元数\n",
    "        :param class_num: int, 出力のクラス数\n",
    "        :param kernel_num: int,　畳み込み層の出力チャネル数\n",
    "        :param kernel_sizes: list of int, カーネルのウィンドウサイズ\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        :param static: bool, 埋め込みを固定するか否かのフラグ\n",
    "        \"\"\"\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.static = static\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # nn.ModuleList: 任意の数のModuleをlistのような形で保持することが出来るクラス\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(1, kernel_num, (kernel_size, embedding_size)) for kernel_size in kernel_sizes]\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, max_length)\n",
    "        x = self.embedding(x)  # (batch_size, max_length, embedding_size)\n",
    "        \n",
    "        if self.static:\n",
    "            x = torch.tensor(x)  # 埋め込みを固定\n",
    "\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, max_length, embedding_size)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, kernel_num, max_length-kernel_size+1), ...]*len(kernel_sizes)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, kernel_num), ...]*len(kernel_sizes)\n",
    "\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*kernel_num)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logit = self.out(x)  # (batch_size, class_num)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(batch_X, batch_Y, model, criterion, optimizer=None, is_train=True):\n",
    "    # バッチの損失を計算\n",
    "    model.train(is_train)\n",
    "    \n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    pred_Y = model(batch_X)\n",
    "    loss = criterion(pred_Y, batch_Y)\n",
    "\n",
    "    pred = torch.max(pred_Y, 1)[1].view(batch_Y.size())\n",
    "    n_correct = (pred.data == batch_Y.data).sum()\n",
    "    \n",
    "    if is_train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item(), n_correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'vocab_size': len(vocab.id2word),\n",
    "    'embedding_size': 128,\n",
    "    'class_num': len(labels),\n",
    "    'kernel_num': 64,\n",
    "    'kernel_sizes': [3, 4, 5],\n",
    "    'dropout': 0.5,\n",
    "    'static': False,\n",
    "}\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "ckpt_path = 'cnn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = TextCNN(**model_args)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_X, train_Y, batch_size)\n",
    "valid_dataloader = DataLoader(valid_X, valid_Y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.7339  train_acc: 49.74  valid_loss: 0.6811  valid_acc: 56.54\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10: train_loss: 0.5159  train_acc: 74.42  valid_loss: 0.6859  valid_acc: 64.56\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15: train_loss: 0.1992  train_acc: 93.55  valid_loss: 0.6523  valid_acc: 72.02\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20: train_loss: 0.0641  train_acc: 98.51  valid_loss: 0.7293  valid_acc: 76.49\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25: train_loss: 0.0283  train_acc: 99.49  valid_loss: 0.8366  valid_acc: 76.26\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30: train_loss: 0.0160  train_acc: 99.71  valid_loss: 0.9395  valid_acc: 76.61\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35: train_loss: 0.0113  train_acc: 99.86  valid_loss: 1.0314  valid_acc: 76.26\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 40: train_loss: 0.0074  train_acc: 99.91  valid_loss: 1.0880  valid_acc: 77.06\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 45: train_loss: 0.0057  train_acc: 99.91  valid_loss: 1.1791  valid_acc: 77.06\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 50: train_loss: 0.0037  train_acc: 99.91  valid_loss: 1.2297  valid_acc: 77.98\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 55: train_loss: 0.0024  train_acc: 99.99  valid_loss: 1.3031  valid_acc: 77.87\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 60: train_loss: 0.0020  train_acc: 99.99  valid_loss: 1.3372  valid_acc: 77.98\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 65: train_loss: 0.0022  train_acc: 99.97  valid_loss: 1.4032  valid_acc: 77.64\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 70: train_loss: 0.0013  train_acc: 99.99  valid_loss: 1.4611  valid_acc: 78.21\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 75: train_loss: 0.0012  train_acc: 99.99  valid_loss: 1.5219  valid_acc: 78.44\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 80: train_loss: 0.0028  train_acc: 99.94  valid_loss: 1.5847  valid_acc: 77.41\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 85: train_loss: 0.0021  train_acc: 99.94  valid_loss: 1.6661  valid_acc: 77.64\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 90: train_loss: 0.0035  train_acc: 99.90  valid_loss: 1.7929  valid_acc: 78.44\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 95: train_loss: 0.0024  train_acc: 99.91  valid_loss: 1.9315  valid_acc: 77.29\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 100: train_loss: 0.0010  train_acc: 99.99  valid_loss: 2.0813  valid_acc: 77.41\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_losses = []\n",
    "    train_corrects = 0\n",
    "    valid_losses = []\n",
    "    valid_corrects = 0\n",
    "    # train\n",
    "    for batch in train_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        train_loss, train_correct = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, optimizer, is_train=True\n",
    "            )\n",
    "        train_losses.append(train_loss)\n",
    "        train_corrects += train_correct\n",
    "    # valid\n",
    "    for batch in valid_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        valid_loss, valid_correct = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, is_train=False\n",
    "            )\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_corrects += valid_correct\n",
    "    train_loss = np.mean(train_losses)\n",
    "    valid_loss = np.mean(valid_losses)\n",
    "    train_acc = 100. * train_corrects / len(train_dataloader.data)\n",
    "    valid_acc = 100. * valid_corrects / len(valid_dataloader.data)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        ckpt = model.state_dict()\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        best_valid_acc = valid_acc\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}: train_loss: {:.4f}  train_acc: {:.2f}  valid_loss: {:.4f}  valid_acc: {:.2f}'.format(\n",
    "                epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "        print('-'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(14830, 128)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3, 128), stride=(1,))\n",
       "    (1): Conv1d(1, 64, kernel_size=(4, 128), stride=(1,))\n",
       "    (2): Conv1d(1, 64, kernel_size=(5, 128), stride=(1,))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (out): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みモデルの読み込み\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.61779242174629\n"
     ]
    }
   ],
   "source": [
    "# テストデータで評価\n",
    "test_dataloader = DataLoader(test_X, test_Y, batch_size)\n",
    "\n",
    "test_corrects = 0\n",
    "for batch in test_dataloader:\n",
    "    batch_X, batch_Y = batch\n",
    "    test_loss, test_correct = compute_loss(\n",
    "        batch_X, batch_Y, model, criterion, is_train=False\n",
    "        )\n",
    "    test_corrects += test_correct\n",
    "\n",
    "test_acc = 100. * test_corrects / len(test_dataloader.data)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際の例を見てみると、正解ラベルが1(positive)である\"if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\"という文をクラス1と分類できていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "true: 1\n",
      "pred: 1\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_X, test_Y, 1)\n",
    "x, y  = next(test_dataloader)\n",
    "\n",
    "pred = model(x).max(1)[1]\n",
    "\n",
    "x = x.cpu().tolist()[0]\n",
    "text = ' '.join([vocab.id2word[i] for i in x][:x.index(0)])\n",
    "print(text)\n",
    "print('true:', y.item())\n",
    "print('pred:', pred.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
