{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 1. VAE 言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 言語モデル\n",
    "言語モデル (Language Model) とは, ある文章 $x$ が生成される過程を$p(x)$として確率的にモデル化したものです. この言語モデルを用いることにより, 文章の尤もらしさを図ったり, またサンプリング$x \\sim p(x)$によって尤もらしい文章を生成したりすることが可能になります.\n",
    "\n",
    "#### Variational Autoencoder (VAE)\n",
    "VAEは, データ$x$の潜在的な意味を表す潜在変数$z$からの生成過程を確率モデル化したものです.\n",
    "潜在変数とは, 例えばMNISTの手書き文字であれば文字であることや筆跡などに当たります.\n",
    "\n",
    "#### VAEを使った言語モデル\n",
    "Bowman et al. (2016) では, このVAEを用いて言語モデルを表現しています.\n",
    "これにより, 文章の背後にある特徴 (文章の意図, スタイル, 作者など) を潜在変数$z$としてモデル化することを試みています.\n",
    "\n",
    "モデルの全体図は次のようになります.\n",
    "\n",
    "<img src=\"./figure/bowman_conll2016.png\">\n",
    "\n",
    "出典: S. R. Bowman et al. \"Generating Sentences from a Continuous Space\". CoNLL. 2016\n",
    "\n",
    "また, 潜在空間に何かしらの意味をもたせることができるようになるため, $z$を潜在空間内で連続的に遷移させたときに一貫性のある補完ができるようになります.\n",
    "参考までに, 下の左図は従来の言語モデルからサンプリングしたもの, 右図は今回のVAEを使った言語モデルからサンプリングしたものになります.\n",
    "\n",
    "<img src=\"./figure/lm.png\" width=\"700mm\">\n",
    "\n",
    "今回はこのVAEを使った言語モデルを2つのRNN (LSTM) を用いて実装していきます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab\n",
    "\n",
    "np.random.seed(34)\n",
    "torch.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "embedding_size = 353 # 単語の埋め込み次元数\n",
    "hidden_size = 191 # LSTMの隠れ層の次元数\n",
    "latent_size = 13  # 潜在変数の次元数\n",
    "\n",
    "min_count = 1\n",
    "\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "\n",
    "TRAIN_X_PATH = 'data/small_tanaka/train_x.txt'\n",
    "VALID_X_PATH = 'data/small_tanaka/valid_x.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は日本語のデータセット (https://github.com/odashi/small_parallel_enja) を使用します.\n",
    "\n",
    "データセットの中身は次のようになっています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 data/small_tanaka/train_x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, n_data=10e+10):\n",
    "    data = []\n",
    "    for i, line in enumerate(open(path, encoding='utf-8')):\n",
    "        words = line.strip().split()\n",
    "        data.append(words)\n",
    "        if i + 1 >= n_data:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data_x, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param data_x: list, 文章 (単語IDのリスト) のリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data_x = data_x\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.data_x = shuffle(self.data_x)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data_x):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "        \n",
    "        # バッチを取得\n",
    "        batch_x = self.data_x[self.start_index:self.start_index+self.batch_size]\n",
    "        \n",
    "        # 系列長で降順にソート\n",
    "        batch_x = sorted(batch_x, key=lambda x: len(x), reverse=True)\n",
    "        \n",
    "        # 系列長を取得\n",
    "        batch_x = [[BOS] + x + [EOS] for x in batch_x]\n",
    "        batch_x_lens = [len(x) for x in batch_x]\n",
    "        \n",
    "        # <S>, </S>を付与 + 短い系列にパディング\n",
    "        max_length = max(batch_x_lens)\n",
    "        batch_x = [x + [PAD] * (max_length - len(x)) for x in batch_x]\n",
    "\n",
    "        # tensorに変換\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.long, device=device)\n",
    "        batch_x_lens = torch.tensor(batch_x_lens, dtype=torch.long, device=device)\n",
    "        \n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "        \n",
    "        return batch_x, batch_x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab({\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}, UNK_TOKEN)\n",
    "\n",
    "sens_train_X = load_data(TRAIN_X_PATH)\n",
    "sens_valid_X = load_data(VALID_X_PATH)\n",
    "\n",
    "vocab.build_vocab(sens_train_X, min_count)\n",
    "\n",
    "train_X = [vocab.sentence_to_ids(sen) for sen in sens_train_X]\n",
    "valid_X = [vocab.sentence_to_ids(sen) for sen in sens_valid_X]\n",
    "\n",
    "vocab_size = len(vocab.word2id)\n",
    "print('語彙数:', vocab_size)\n",
    "print('学習用データ数:', len(train_X))\n",
    "print('検証用データ数:', len(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_X, batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(valid_X, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論モデル(Encoder)をLSTM + MLP, 生成モデル(Decoder)をLSTMで実装します.\n",
    "\n",
    "EncoderはLSTMによって入力文をエンコードした後2つのMLPによってそれぞれガウス分布$\\mu$の平均と分散$\\sigma^2$を計算し, ガウス分布$\\mathcal{N}(\\mu, \\sigma^2)$からサンプリングされた$z$を出力します.\n",
    "\n",
    "Decoderは初期状態として潜在変数$z$を受け取り, LSTMによって文を生成(デコード)していきます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/bowman_conll2016.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出典: S. R. Bowman et al. \"Generating Sentences from a Continuous Space\". CoNLL. 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は次の式のようになります. 第一項は再構成誤差とよばれ負の対数尤度に当たります. 第二項は正則化項で事後分布$q_\\theta(z|x)$を事前分布$p(z)$に近づける役割を果たします."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{L}(\\theta;x) = - \\mathbb{E}_{q_{\\theta}(z|x)} \\left[\\log{p_{\\theta}(x|z)}\\right] + D_{\\mathrm{KL}}\\left(q_{\\theta}(z|x)||p(z)\\right)\n",
    "$$\n",
    "\n",
    "第二項のKLダイバージェンスの式は次のようになります.\n",
    "\n",
    "$$\n",
    "    D_{\\mathrm{KL}} \\left(q_{\\phi} (z|x) || p(z) \\right) = -\\frac{1}{2} \\sum^M_{m=1} \\left( 1 + \\log \\sigma^2_m - \\mu^2_m - \\sigma^2_m \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. 学習トリック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1. KL項のアニーリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数においてKL項の学習は対数尤度のそれよりも簡単です. そのため上記の式のままだと早い段階でKL項の損失が0になり, 潜在変数$z$を考慮しない普通の言語モデルと同じになってしまいます (posterior collapse) .\n",
    "\n",
    "これを防ぐため, KL項の大きさを係数$\\lambda$によってコントロールします. 学習の初期段階では小さい値(0)を設定し正則化の力を弱め, 潜在変数$z$にできるだけ多くの情報がencodeされるように仕向けます. そして学習ステップが進むにつれて$\\lambda$を1に近づけていき, 本来のガウス分布に埋め込まれるようにしていきます.\n",
    "\n",
    "この関数を`get_kl_weight`として実装します.\n",
    "$\\lambda$の計算の仕方としては, sigmoidやtanh関数を使って滑らかに推移させていく方法や, 0から1に線形に増加させていく方法などがあります. 初出のBowmanの論文では前者を用いており, 最近の論文では後者のほうが多い印象です.\n",
    "\n",
    "今回は原論文に従って前者で実装を行います.\n",
    "アニーリングの関数は次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_weight(step):\n",
    "    \"\"\"step数でアニーリングしたKL項の重みを取得する (0 -> 1)\n",
    "    \"\"\"\n",
    "    return (math.tanh((step - 3500)/1000) + 1) /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この係数$\\lambda$の推移のイメージは次のグラフのようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.arange(10000)\n",
    "ys = np.array([get_kl_weight(x) for x in xs])\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.xlabel(r'学習のステップ数')\n",
    "plt.ylabel(r'KL項の重み')\n",
    "plt.xlim(0, 10000)\n",
    "plt.ylim(0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2. Word dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderは潜在変数$z$だけでなく元の文$x$も入力として受け取ります.\n",
    "そのためDecoderの関数としての表現力が強いと$z$を無視しても学習が進むようになってしまい, 潜在変数$z$を考慮した文生成ができなくなってしまいます.\n",
    "\n",
    "これを防ぐため, Decoderの入力単語を一定の確率$p$で`<UNK>`に置き換えます.\n",
    "これにより, 潜在変数$z$をより頼りにして学習を進めるようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_drop_rate = 0.38 # 単語を<UNK>に置き換える確率 (論文では 0.38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際のDecoderクラス内の該当箇所の実装は次のようになっています."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Decoder:\n",
    "    ...\n",
    "        # 単語を一定確率pで<UNK>に置き換える\n",
    "        if self.training:\n",
    "            prob = torch.full_like(x, self.word_drop_rate, dtype=torch.float32).to(device)\n",
    "            mask = torch.bernoulli(prob)\n",
    "            x = torch.where(mask == 1, torch.full_like(x, UNK).to(device), x)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力言語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_size: int, 洗剤変数zの次元数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_m = nn.Linear(hidden_size, latent_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, latent_size)\n",
    "    \n",
    "    def forward(self, x, x_lens=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 入力のバッチ, size=(バッチサイズ, 系列長)\n",
    "        :param x_lens: tensor, 入力の系列長, size=(バッチサイズ, 系列長)\n",
    "        :return z: tensor, サンプリングした潜在変数, size=(バッチサイズ, latent_size)\n",
    "        :return mean: tensor, 潜在変数の平均, size=(バッチサイズ, latent_size)\n",
    "        :return lvar: tensor, 潜在変数のlog分散, size=(バッチサイズ, latent_size)\n",
    "        :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) # 単語の埋め込み\n",
    "        if x_lens is not None:\n",
    "            x = pack_padded_sequence(x, lengths=x_lens, batch_first=True)\n",
    "        \n",
    "        _, (h_T, _) = self.lstm(x)\n",
    "        \n",
    "        h_T.squeeze_()\n",
    "        \n",
    "        mean = self.linear_m(h_T) # 平均\n",
    "        lvar = self.linear_v(h_T) # 分散のlog\n",
    "        std = torch.exp(0.5 * lvar) # 標準偏差\n",
    "        \n",
    "        eps = torch.randn(mean.size()).to(device) # 標準正規分布からサンプリング\n",
    "        z = mean + std * eps # 潜在変数\n",
    "        \n",
    "        loss_kl = # WRITE ME # KL divergenceの計算\n",
    "        \n",
    "        return z, mean, lvar, loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderクラスでは次の2種類の関数を実装します.\n",
    "\n",
    "##### forward関数\n",
    "潜在変数$z$を元に, RNN (LSTM) により元の文$x$を復元できるよう一単語ずつ出力していきます.\n",
    "また通常の言語モデルや翻訳のときと同様に, 正解データ$x$の単語をRNN (LSTM) の各時刻の入力とします (Teacher Forcing).\n",
    "\n",
    "##### sample関数\n",
    "潜在変数$z$のみを受け取り, RNN (LSTM) により新しい文$\\hat{x}$を生成していきます.\n",
    "また各時刻で単語を出力する際に, softmaxの確率分布からサンプリングするか, 最大値を取る単語を出力するかを`greedy_decoding`オプションにより切り替えます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_size, word_drop_rate):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力単語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_size: int, 潜在変数zの次元数\n",
    "        :param word_drop_rate: int, 入力単語をunkに置き換える確率\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size+latent_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_h = nn.Linear(latent_size, hidden_size)\n",
    "        self.linear_c = nn.Linear(latent_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_drop_rate = word_drop_rate\n",
    "        \n",
    "        self.word_drop = nn.Dropout(word_drop_rate)\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 入力単語のバッチ, size=(バッチサイズ, 系列長)\n",
    "        :param z: tensor, 潜在変数のバッチ, size=(バッチサイズ, latent_size)\n",
    "        :return y: tensor, Decoderの出力, size=(バッチサイズ, 系列長, 語彙数)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(z).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(z).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        \n",
    "        # 単語を一定確率pで<UNK>に置き換える\n",
    "        if self.training:\n",
    "            prob = torch.full_like(x, self.word_drop_rate, dtype=torch.float32).to(device)\n",
    "            mask = torch.bernoulli(prob)\n",
    "            x = torch.where(mask == 1, torch.full_like(x, UNK).to(device), x)\n",
    "        \n",
    "        # デコード\n",
    "        x = self.embedding(x) # 単語の埋め込み\n",
    "        x = torch.cat([x, z.unsqueeze(1).repeat(1, x.shape[1], 1)], dim=2) # 潜在変数zを毎時刻単語のembeddingにconcatする\n",
    "\n",
    "        h, (_, _) = self.lstm(x, (h_0, c_0)) # LSTM\n",
    "        \n",
    "        y = self.out(h)\n",
    "        \n",
    "        return y # (バッチサイズ, 系列長, 語彙数)\n",
    "    \n",
    "    def sample(self, z, max_length, greedy_decoding=True):\n",
    "        \"\"\"\n",
    "        :param z: tensor, 潜在変数のバッチ, size=(バッチサイズ, latent_size)\n",
    "        :param max_length: int, 生成文のmax長\n",
    "        :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(z).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(z).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, cellの次元数)\n",
    "        \n",
    "        # 最初の単語は<S>\n",
    "        x_0 = torch.full((z.size(0), 1), BOS, dtype=torch.long).to(device) # (バッチサイズ, 1)\n",
    "        \n",
    "        z = z.unsqueeze(1) # size=(バッチサイズ, 1, latent_size)\n",
    "        \n",
    "        x_tm1, h_tm1, c_tm1 = x_0, h_0, c_0\n",
    "        \n",
    "        x_hat = [] # 生成文を格納するlist\n",
    "        \n",
    "        flag = np.zeros(x_0.size(0), dtype=bool) # 出力が終わったかどうかのflag\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            x_t = self.embedding(x_tm1) # 単語の埋め込み, size=(バッチサイズ, 1, embedding_size)\n",
    "            x_t = torch.cat([x_t, z], dim=2) # 潜在変数と単語埋め込みをconcat, size=(バッチサイズ, 1, embedding_size+latent_size)\n",
    "            \n",
    "            _, (h_t, c_t) = self.lstm(x_t, (h_tm1, c_tm1)) # LSTM\n",
    "            \n",
    "            y_t = F.softmax(self.out(h_t[0]), dim=-1) # Softmax, size=(バッチサイズ, 語彙数)\n",
    "            \n",
    "            if greedy_decoding:\n",
    "                # 確率の一番高い単語を取得する\n",
    "                x_t = y_t.argmax(1).unsqueeze(1)\n",
    "            else:\n",
    "                # 確率分布からサンプリングする\n",
    "                x_t = torch.multinomial(y_t, 1)\n",
    "            \n",
    "            x_hat.append(x_t)\n",
    "            \n",
    "            # t -> t-1\n",
    "            x_tm1, h_tm1, c_tm1 = x_t, h_t, c_t\n",
    "            \n",
    "            # </S>が出力されたらFlagを更新する\n",
    "            flag_t = (x_t.squeeze().cpu().numpy() == EOS)\n",
    "            flag = np.logical_or(flag, flag_t)\n",
    "            \n",
    "            # すべての系列で</S>が出力されたら終了\n",
    "            if np.all(flag):\n",
    "                break\n",
    "        \n",
    "        # listからtensorに変換する\n",
    "        x_hat = torch.cat(x_hat, dim=1)\n",
    "        \n",
    "        return x_hat # size=(バッチサイズ, 系列長)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_size': latent_size\n",
    "}\n",
    "\n",
    "D_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_size': latent_size,\n",
    "    'word_drop_rate': word_drop_rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cudnnのエラーが出る場合は, このセルをもう一度実行してください\n",
    "E = Encoder(**E_args).to(device)\n",
    "D = Decoder(**D_args).to(device)\n",
    "\n",
    "optimizer_E = optim.Adam(E.parameters())\n",
    "optimizer_D = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. その他の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_prior(batch_size):\n",
    "    \"\"\"事前分布p(z)からzをサンプリングする\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :return z: tensor, サンプリングされたz, size=(バッチサイズ, latent_size)\n",
    "    \"\"\"\n",
    "    z = torch.randn(batch_size, latent_size).to(device)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 言語モデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "負の対数尤度及びPerplexityにより行います.\n",
    "ともに値が低いほど良い言語モデルと言えます.\n",
    "\n",
    "##### 負の対数尤度\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathrm{NLL}(x) &= -\\log \\prod^N_{n=1}p_{\\theta} (x_n|x_{<n}, z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### Perplexity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathrm{PPL}(x) &= \\left(\\prod^N_{n=1} p_{\\theta}(x|z)\\right)^{-\\frac{1}{N}} \\\\\n",
    "    &= \\left(e^{-\\mathrm{NLL}(x)}\\right)^{-\\frac{1}{N}} \\\\\n",
    "    &= e^{\\frac{\\mathrm{NLL}(x)}{N}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_vae(x, x_lens, lmd, is_train=False):\n",
    "    \"\"\"VAEの損失関数 (負の対数尤度 + lmd * KLダイバージェンス) を計算する\n",
    "    :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_len: tensor, 入力単語の系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param lmd: int, KLダイバージェンスの大きさをコントロールする係数\n",
    "    :param is_train: bool, モデル (EとD) のパラメータを更新するか否か\n",
    "    :return loss_vae: tensor, VAEの損失, size=()\n",
    "    :return nll: tensor, 負の対数尤度, size=()\n",
    "    :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "    :return ppl: tensor, パープレキシティ, size=()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encoder\n",
    "    input_encoder = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    input_encoder_lens = x_lens - 1\n",
    "    z, mean, lvar, loss_kl = E.forward(input_encoder, input_encoder_lens)\n",
    "    \n",
    "    # Decoder\n",
    "    input_decoder = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    target_decoder = x[:, 1:].contiguous() # [x_1, x_2, ..., </S>]\n",
    "    output_decoder = D.forward(input_decoder, z)\n",
    "    \n",
    "    # 損失\n",
    "    nll_all = F.cross_entropy(output_decoder.view(-1, vocab_size), target_decoder.view(-1), ignore_index=PAD, size_average=False, reduce=False)\n",
    "    nll_mb = torch.sum(nll_all.view(output_decoder.size(0), output_decoder.size(1)), dim=1)\n",
    "    nll = nll_mb.mean()\n",
    "    ppl_mb = torch.exp(nll_mb / input_encoder_lens.type(torch.float))\n",
    "    ppl = ppl_mb.mean()\n",
    "    \n",
    "    loss_vae = nll + lmd * loss_kl\n",
    "    \n",
    "    if is_train:\n",
    "        E.zero_grad()\n",
    "        D.zero_grad()\n",
    "        loss_vae.backward()\n",
    "        optimizer_E.step()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    return loss_vae, nll, loss_kl, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    E.train()\n",
    "    D.train()\n",
    "    nll_train = []\n",
    "    kl_train = []\n",
    "    ppl_train = []\n",
    "    for batch_x, batch_x_lens in dataloader_train:\n",
    "        lmd = get_kl_weight(step)\n",
    "        \n",
    "        # 損失の計算 & パラメータの更新\n",
    "        loss, nll, loss_kl, ppl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=True)\n",
    "        nll_train.append(nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "        ppl_train.append(ppl.item())\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    # Valid\n",
    "    E.eval()\n",
    "    D.eval()\n",
    "    nll_valid = []\n",
    "    kl_valid = []\n",
    "    ppl_valid = []\n",
    "    for batch_x, batch_x_lens in dataloader_valid:\n",
    "        # 損失の計算\n",
    "        loss, nll, loss_kl, ppl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=False)\n",
    "        nll_valid.append(nll.item())\n",
    "        kl_valid.append(loss_kl.item())\n",
    "        ppl_valid.append(ppl.item())\n",
    "\n",
    "    print('EPOCH: {}, LMD: {:.2f}, Train [NLL: {:.3f}, KL: {:.3f}, PPL: {:.3f}], Valid [NLL: {:.3f}, KL: {:.3f}, PPL: {:.3f}], Elapsed Time: {:.2f}[s]'.format(\n",
    "        epoch + 1,\n",
    "        lmd,\n",
    "        np.mean(nll_train),\n",
    "        np.mean(kl_train),\n",
    "        np.mean(ppl_train),\n",
    "        np.mean(nll_valid),\n",
    "        np.mean(kl_valid),\n",
    "        np.mean(ppl_valid),\n",
    "        time.time() - start_time\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. ランダム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前分布$p(z)$からサンプリングした$z$により文を生成してみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(batch_size, max_length):\n",
    "    \"\"\"Decoderから文を生成する\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :param max_length: int, 生成文のmax長\n",
    "    :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    x_hat = D.sample(z, max_length)\n",
    "    \n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 30\n",
    "max_length = 20\n",
    "\n",
    "x = sample(n_samples, max_length)\n",
    "x = x.cpu().numpy()\n",
    "\n",
    "for i, x_i in enumerate(x):\n",
    "    x_i = ' '.join([vocab.id2word[i] for i in x_i])\n",
    "    x_i = re.sub(r' {}.*'.format(EOS_TOKEN), '', x_i) # </S>以降は除去\n",
    "    print('{}. {}'.format(i, x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Interporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前分布$p(z)$からサンプリングしたデータ間を遷移したときの生成文の変化を確認してみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "z1 = sample_z_prior(1).squeeze()\n",
    "z2 = sample_z_prior(1).squeeze()\n",
    "\n",
    "interpolation = torch.zeros((latent_size, n)).to(device)\n",
    "\n",
    "for i, (z1_i, z2_i) in enumerate(zip(z1, z2)):\n",
    "    interpolation[i] = torch.linspace(z1_i, z2_i, n).to(device)\n",
    "\n",
    "interpolation.t_()\n",
    "\n",
    "x_hat = D.sample(interpolation, max_length).cpu().numpy()\n",
    "\n",
    "for i, x_i in enumerate(x_hat):\n",
    "    x_i = ' '.join([vocab.id2word[i] for i in x_i])\n",
    "    x_i = re.sub(r' {}.*'.format(EOS_TOKEN), '', x_i) # </S>以降は除去\n",
    "    print('{}. {}'.format(i, x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考資料\n",
    "\n",
    "- 原論文: http://www.aclweb.org/anthology/K16-1002\n",
    "- Neural Networks for NLP - Latent Random Variable - (CMU): http://www.phontron.com/class/nn4nlp2018/schedule/latent-random-variables.html\n",
    "- PyTorch実装 (非公式) : https://github.com/timbmg/Sentence-VAE\n",
    "- TensorFlow実装 (非公式) : https://github.com/ryokamoi/original_textvae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
