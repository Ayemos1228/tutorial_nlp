{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 2. 文章の属性を制御して文章を生成するモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの概要\n",
    "前回の課題のVAE言語モデルでは文章を生成することはできましたが, 文章の属性 (極性, 時制, 長さ, 丁寧さ, etc.) は制御することができませんでいた.\n",
    "\n",
    "今回の課題では, Z. Hu et al. (2017) で提案された, **属性を制御しながら文章を生成することのできるモデル**を実装します.\n",
    "モデルの全体像は次の通りになっています.\n",
    "\n",
    "<img src=\"figure/hu_icml2017.png\">\n",
    "\n",
    "出典: Z. Hu et al. \"Toward Controlled Generation of Text\", ICML, 2017\n",
    "\n",
    "Encoder, GeneratorはVAE言語モデルのものとほぼ同じです (Generatorは前課題のDecoderと同じですが, もとの論文に合わせてGeneratorと呼んでいます) が, **Generatorが生成した文の属性に対してフィードバックを行うDiscriminator**が新たに追加されています.\n",
    "\n",
    "また, 前回のVAE言語モデルでは潜在変数は$z$と一括りに扱っていましたが, ここでは制御したい属性に対する潜在変数を**潜在コード$c$**として$z$から独立させます.\n",
    "潜在変数$z$は前回と同様ガウス分布によりモデリングしますが, 潜在コード$c$は入力文$x$に紐づけされた属性のラベルをone_hot化して使用します.\n",
    "\n",
    "Discriminatorは制御したい属性の種類分用意する必要があり, 例えばもとの論文では2つのDiscriminatorを学習させることにより文章の時制と極性を同時に制御できるモデルを構築しています.\n",
    "本課題では簡単のためにDiscriminatorは1つだけ使用します.\n",
    "\n",
    "半教師ありのVAEと同じタスク設定ですが, それよりも良い結果がでることが実験で示されています.\n",
    "\n",
    "\n",
    "### 学習について\n",
    "学習は2ステップに分かれています.\n",
    "\n",
    "第1段階では属性のついていないテキストデータに対して通常のVAE (Encoder + Generator) の学習を行います. この段階では属性ラベル$c$は使用せず, 事前分布$p(c)$からサンプリングしたものを使用します.\n",
    "\n",
    "第2段階で属性付きのテキストデータを利用し, モデルが属性$c$に沿った文を生成できるようにEncoder, Generator, Discriminatorをそれぞれ学習させていきます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab\n",
    "\n",
    "np.random.seed(34)\n",
    "torch.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "embedding_size = 300 # 単語の埋め込み次元数\n",
    "hidden_size = 300 # LSTMの隠れ層次元数\n",
    "latent_z_size = 32  # 潜在変数の次元数\n",
    "latent_c_size = 2   # 潜在コードの次元数\n",
    "latent_size = latent_z_size + latent_c_size\n",
    "n_filters = 100 # Discriminator (CNN) のフィルター数\n",
    "\n",
    "max_length = 11\n",
    "min_count = 1 # 出現数がMIN_COUNT未満の単語は<UNK>に置き換える\n",
    "\n",
    "word_drop_rate = 0.5\n",
    "\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "\n",
    "beta = 0.1\n",
    "lmd_c = 0.1\n",
    "lmd_u = 0.1\n",
    "lmd_z = 0.1\n",
    "\n",
    "TRAIN_X_PATH = 'data/styletransfer/train_x.txt'\n",
    "TRAIN_Y_PATH = 'data/styletransfer/train_y.txt'\n",
    "VALID_X_PATH = 'data/styletransfer/valid_x.txt'\n",
    "VALID_Y_PATH = 'data/styletransfer/valid_y.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は, Z. Fu et al. (2017) で提案されている商品レビューのデータセットを使用します.\n",
    "このデータセットでは800,000件のレビューに対して, その内容が肯定的 (1) なのか否定的 (0) なのかがラベル付されています.\n",
    "このラベルをDiscriminatorで制御する属性として扱います.\n",
    "\n",
    "演習では系列長9~10のデータ約8万件に絞って使用します.\n",
    "データはすべてgithub (https://github.com/fuzhenxin/textstyletransferdata) で公開されているので, 興味と時間のある人は見てみてください.\n",
    "\n",
    "出典: Z. Fu et al. \"Style Transfer in Text: Exploration and Evaluation\", AAAI (2018)\n",
    "\n",
    "データセットの中身は次のようになっています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 data/styletransfer/train_x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 data/styletransfer/train_y.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, n_data=10e+10):\n",
    "    data = []\n",
    "    for i, line in enumerate(open(path, encoding='utf-8')):\n",
    "        words = line.strip().split()\n",
    "        data.append(words)\n",
    "        if i + 1 >= n_data:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data_x, data_y, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param data_x: list, 文章 (単語IDのリスト) のリスト\n",
    "        :param data_y: list, 属性ラベルのリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data = list(zip(data_x, data_y))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.data = shuffle(self.data)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "        \n",
    "        # バッチを取得\n",
    "        batch_x, batch_c = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
    "        \n",
    "        # 系列長で降順にソート\n",
    "        batch = sorted(zip(batch_x, batch_c), key=lambda x: len(x[0]), reverse=True)\n",
    "        batch_x, batch_c = zip(*batch)\n",
    "        \n",
    "        # 系列長を取得\n",
    "        batch_x = [[BOS] + x + [EOS] for x in batch_x]\n",
    "        batch_x_lens = [len(x) for x in batch_x]\n",
    "        \n",
    "        # <S>, </S>を付与 + 短い系列にパディング\n",
    "        max_length = max(batch_x_lens)\n",
    "        batch_x = [x + [PAD] * (max_length - len(x)) for x in batch_x]\n",
    "\n",
    "        # tensorに変換\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.long, device=device)\n",
    "        batch_c = torch.tensor(batch_c, dtype=torch.long, device=device)\n",
    "        batch_x_lens = torch.tensor(batch_x_lens, dtype=torch.long, device=device)\n",
    "        \n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "        \n",
    "        return batch_x, batch_c, batch_x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab({\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}, UNK_TOKEN)\n",
    "\n",
    "sens_train_X = load_data(TRAIN_X_PATH)\n",
    "sens_valid_X = load_data(VALID_X_PATH)\n",
    "\n",
    "vocab.build_vocab(sens_train_X, min_count)\n",
    "\n",
    "train_X = [vocab.sentence_to_ids(sen) for sen in sens_train_X]\n",
    "valid_X = [vocab.sentence_to_ids(sen) for sen in sens_valid_X]\n",
    "\n",
    "train_Y = np.loadtxt(TRAIN_Y_PATH)\n",
    "valid_Y = np.loadtxt(VALID_Y_PATH)\n",
    "\n",
    "vocab_size = len(vocab.word2id)\n",
    "print('語彙数:', vocab_size)\n",
    "print('学習用データ数:', len(train_X))\n",
    "print('検証用データ数:', len(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_X, train_Y, batch_size)\n",
    "dataloader_valid = DataLoader(valid_X, valid_Y, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. 単語の確率分布に対する単語埋め込みベクトルの取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語埋め込みベクトルの計算時に, 離散的な単語IDからだけではなく, 連続的な確率分布に対しても計算を行えるよう実装を行います. 実装は単純な行列積として表します.\n",
    "\n",
    "イメージは下の図のようになります.\n",
    "通常の単語の埋め込み操作では特定のk番目の単語ベクトルのみを取り出している (左図) のに対し, 各単語の確率の値に対する総和として埋め込みベクトルを表現 (右図) しています.\n",
    "\n",
    "<img src=\"figure/continuous_approximation.png\">\n",
    "\n",
    "これにより, モデル内における勾配の伝播が行いやすくなります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE言語モデルで実装したものとほぼ同じですが, 確率分布に対しても単語埋め込みベクトルの計算を行えるよう実装を修正します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_z_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力単語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_z_size: int, 潜在変数zの次元数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_m  = nn.Linear(hidden_size, latent_z_size)\n",
    "        self.linear_v  = nn.Linear(hidden_size, latent_z_size)\n",
    "    \n",
    "    def forward(self, x, x_lens=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 単語id列のバッチ or 単語の確率分布列のバッチ, size=(バッチサイズ, 系列長) or (バッチサイズ, 系列長, 語彙数)\n",
    "        :param x_lens: tensor, 単語id列の長さのバッチ, size=(バッチサイズ, 系列長)\n",
    "        :return z: tensor, サンプリングした潜在変数, size=(バッチサイズ, latent_size)\n",
    "        :return mean: tensor, 潜在変数の平均, size=(バッチサイズ, latent_size)\n",
    "        :return lvar: tensor, 潜在変数のlog分散, size=(バッチサイズ, latent_size)\n",
    "        :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "        \"\"\"\n",
    "        if x.dim() == 2: # xが単語IDのtensorの場合\n",
    "            x = self.embedding(x)\n",
    "            if x_lens is not None:\n",
    "                x = pack_padded_sequence(x, lengths=x_lens, batch_first=True) # <PAD>の部分は無視してエンコードする\n",
    "        elif x.dim() == 3: # xが単語の確率分布のtensorの場合\n",
    "            x = # WRITE ME\n",
    "\n",
    "        _, (h_T, _) = self.lstm(x) # LSTM\n",
    "        \n",
    "        mean = self.linear_m(h_T[0]) # 平均\n",
    "        lvar = self.linear_v(h_T[0]) # 分散のlog\n",
    "        std = torch.exp(0.5 * lvar) # 標準偏差\n",
    "        \n",
    "        eps = torch.randn(mean.size()).to(device) # 標準正規分布からサンプリング\n",
    "        z = mean + std * eps # 潜在変数\n",
    "        \n",
    "        loss_kl = - 0.5 * torch.mean(torch.sum(1 + lvar - mean**2 - lvar.exp(), dim=1)) # KLダイバージェンスの計算\n",
    "\n",
    "        return z, mean, lvar, loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE言語モデルで実装したものとほぼ同じですが, Encoderと同様に, 確率分布に対しても単語埋め込みベクトルの計算を行えるよう実装を修正します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_size, word_drop_rate):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力単語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_size: int, 潜在変数zの次元数\n",
    "        :param word_drop_rate: int, 入力単語をunkに置き換える確率\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size+latent_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_h = nn.Linear(latent_size, hidden_size)\n",
    "        self.linear_c = nn.Linear(latent_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_drop_rate = word_drop_rate\n",
    "\n",
    "    def forward(self, x, zc):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "        :param zc: tensor, 潜在変数と潜在コードのバッチ, size=(バッチサイズ, latent_z_size+latent_size)\n",
    "        :return y: tensor, Decoderの出力, size=(バッチサイズ, 系列長, 語彙数)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(zc).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(zc).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        \n",
    "        # 単語を一定確率pで<unk>に置き換える\n",
    "        if self.training:\n",
    "            prob = torch.full_like(x, self.word_drop_rate, dtype=torch.float32).to(device)\n",
    "            mask = torch.bernoulli(prob)\n",
    "            x = torch.where(mask == 1, torch.full_like(x, UNK).to(device), x)\n",
    "        \n",
    "        x = self.embedding(x) # 単語の埋め込み\n",
    "        x = torch.cat([x, zc.unsqueeze(1).repeat(1, x.shape[1], 1)], dim=2) # 潜在変数+コードzcを毎時刻単語のembeddingにconcatする\n",
    "        \n",
    "        h, (_, _) = self.lstm(x, (h_0, c_0)) # LSTM\n",
    "        \n",
    "        y = self.out(h) # size=(バッチサイズ, 系列長, 語彙数)\n",
    "        \n",
    "        return y # size=(バッチサイズ, 系列帳, 語彙数)\n",
    "    \n",
    "    def sample(self, zc, max_length, soft_decoding, greedy_decoding):\n",
    "        \"\"\"\n",
    "        :param zc: tensor, 潜在変数と潜在コードのバッチ, size=(バッチサイズ, latent_size)\n",
    "        :param max_length: int, 生成文の最大長\n",
    "        :param soft_decoding: bool, 各時刻の出力を単語IDとするか単語の確率分布とするか\n",
    "        :param greedy_decoding: bool, 各時刻で確率の最も高い単語を取得するか確率分布からサンプリングするか\n",
    "        :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(zc).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(zc).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        \n",
    "        # 最初の単語は<S>\n",
    "        x_0 = torch.full((zc.size(0), 1), BOS, dtype=torch.long).to(device) # size=(バッチサイズ, 1)\n",
    "\n",
    "        if soft_decoding:\n",
    "            # 単語IDをone-hot化, size=(バッチサイズ, 語彙数)\n",
    "            x_0 = torch.zeros((x_0.size(0), self.vocab_size), device=device).scatter(1, x_0, 1.0)\n",
    "    \n",
    "        zc = zc.unsqueeze(1) # size=(バッチサイズ, 1, latent_size)\n",
    "        \n",
    "        x_tm1, h_tm1, c_tm1 = x_0, h_0, c_0\n",
    "\n",
    "        x_generated = [] # 生成文を格納するlist\n",
    "        \n",
    "        if not soft_decoding:\n",
    "            flag = np.zeros(x_0.size(0), dtype=bool) # 出力が終わったかどうかのFlag\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if soft_decoding: # 単語の確率分布から埋め込みベクトルを取得\n",
    "                x_t = torch.matmul(x_tm1, self.embedding.weight).unsqueeze(1) # size=(バッチサイズ, 1, embedding_size)\n",
    "            else: # 単語IDから埋め込みベクトルを取得\n",
    "                x_t = self.embedding(x_tm1) # size=(バッチサイズ, 1, embedding_size)\n",
    "            \n",
    "            x_t = torch.cat([x_t, zc], dim=2) # 潜在変数+潜在コードと単語埋め込みベクトルを連結, size=(バッチサイズ, 1, embedding_size+latent_size)\n",
    "            \n",
    "            _, (h_t, c_t) = self.lstm(x_t, (h_tm1, c_tm1)) # LSTM\n",
    "            \n",
    "            y_t = F.softmax(self.out(h_t[0]), dim=-1) # Softmax, size=(バッチサイズ, 語彙数)\n",
    "            \n",
    "            if not soft_decoding:\n",
    "                if greedy_decoding: # 確率の一番高い単語を取得する\n",
    "                    y_t = y_t.argmax(1).unsqueeze(1)\n",
    "                else: # 確率分布からサンプリングする\n",
    "                    y_t = torch.multinomial(y_t, 1) # (バッチサイズ, 1)\n",
    "            \n",
    "            x_generated.append(y_t)\n",
    "\n",
    "            # t -> t-1\n",
    "            x_tm1, h_tm1, c_tm1 = y_t, h_t, c_t\n",
    "\n",
    "            if not soft_decoding:\n",
    "                # </S>が出力されたらFlagを更新する\n",
    "                flag_t = (y_t.squeeze().cpu().numpy() == EOS)\n",
    "                flag = np.logical_or(flag, flag_t)\n",
    "\n",
    "                # Bすべての系列で</S>が出力されたら終了\n",
    "                if np.all(flag):\n",
    "                    break\n",
    "\n",
    "        # listからtensorに変換する\n",
    "        if soft_decoding:\n",
    "            x_generated = torch.stack(x_generated, dim=1)\n",
    "        else:\n",
    "            x_generated = torch.cat(x_generated, dim=1)\n",
    "\n",
    "        return x_generated # size=(バッチサイズ, 系列長, 語彙数) or (バッチサイズ, 系列長)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generatorから生成された文の属性を判別するDisciriminatorをCNNで実装します.\n",
    "これはChapter 4で実装したものと同じです.\n",
    "\n",
    "これに関しても確率分布に対する単語埋め込みベクトルの計算を追加で実装します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, n_filters, latent_c_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv_1 = nn.Conv1d(embedding_size, n_filters, kernel_size=3)\n",
    "        self.conv_2 = nn.Conv1d(embedding_size, n_filters, kernel_size=4)\n",
    "        self.conv_3 = nn.Conv1d(embedding_size, n_filters, kernel_size=5)\n",
    "        self.out = nn.Linear(n_filters*3, latent_c_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 単語id列のバッチ or 単語の確率分布列のバッチ, size=(バッチサイズ, 系列長) or (バッチサイズ, 系列長, 語彙数)\n",
    "        :return x: tensor, size=(バッチサイズ, latent_c_size)\n",
    "        \"\"\"\n",
    "        x_len = x.size(1)\n",
    "\n",
    "        if x.dim() == 2: # xが単語IDのtensorの場合\n",
    "            x = self.embedding(x)\n",
    "        elif x.dim() == 3: # xが単語の確率分布のtensorの場合\n",
    "            x = torch.matmul(x, self.embedding.weight)\n",
    "\n",
    "        x = x.permute(0, 2, 1) # conv用に次元を入れ替え, size=(バッチサイズ, embedding_size, 系列長)\n",
    "\n",
    "        x1 = torch.tanh(self.conv_1(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-2)\n",
    "        x2 = torch.tanh(self.conv_2(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-3)\n",
    "        x3 = torch.tanh(self.conv_3(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-4)\n",
    "\n",
    "        x1 = F.max_pool1d(x1, x_len-2) # x1に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "        x2 = F.max_pool1d(x2, x_len-3) # x2に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "        x3 = F.max_pool1d(x3, x_len-4) # x3に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=1)[:, :, 0] # size=(バッチサイズ, フィルター数*3)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x # (バッチサイズ, latent_c_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_z_size': latent_z_size\n",
    "}\n",
    "\n",
    "G_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_size': latent_size,\n",
    "    'word_drop_rate': word_drop_rate\n",
    "}\n",
    "\n",
    "D_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'n_filters': n_filters,\n",
    "    'latent_c_size': latent_c_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder(**E_args).to(device) # cudnnのエラーが出る場合はこのセルをもう一度実行してください\n",
    "G = Generator(**G_args).to(device)\n",
    "D = Discriminator(**D_args).to(device)\n",
    "\n",
    "optimizer_E = optim.Adam(E.parameters())\n",
    "optimizer_G = optim.Adam(G.parameters())\n",
    "optimizer_D = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. その他の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_prior(batch_size):\n",
    "    \"\"\"事前分布p(z)からzをサンプリングする\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :return z: tensor, サンプリングされたz, size=(バッチサイズ, latent_z_size)\n",
    "    \"\"\"\n",
    "    z = torch.randn(batch_size, latent_z_size, device=device)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_c_prior(batch_size):\n",
    "    \"\"\"事前分布p(c)からcをサンプリングする\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :return c: tensor, サンプリングされたc, size=(バッチサイズ, latent_c_size)\n",
    "    \"\"\"\n",
    "    weights = torch.ones(latent_c_size, dtype=torch.float).to(device)\n",
    "    c = torch.multinomial(weights, batch_size, replacement=True)\n",
    "    c = torch.eye(latent_c_size)[c].to(device) # one_hot化\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 損失関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全部で5つの損失関数を実装します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1. $\\mathcal{L}_{VAE}$\n",
    "\n",
    "VAE言語モデルの損失関数と同じです. 係数$\\lambda$でKL項の強さをコントロールします.\n",
    "\n",
    "第1段階, 第2段階の学習で使用します.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VAE}(\\theta_G, \\theta_E; x) = -\\mathbb{E}_{q_E(z|x) q_D(c|x)} \\left[\\log p_G(x|z, c)\\right] + \\lambda \\cdot D_{\\mathrm{KL}}\\left(q_E(z|x)||p(z)\\right)\n",
    "$$\n",
    "\n",
    "<img src=\"figure/loss_vae.png\" width=\"800mm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_vae(x, x_lens, lmd, use_c_prior=True, is_train=False):\n",
    "    \"\"\"VAEの損失関数 (負の対数尤度 + lmd * KLダイバージェンス) を計算する\n",
    "    :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_len: tensor, 入力単語の系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param lmd: int, KLダイバージェンスの大きさをコントロールする係数\n",
    "    :param use_c_prior: bool, 潜在コードcを事前分布p(c)からサンプリングする か Discriminatorからの出力を利用するか\n",
    "    :param is_train: bool, モデル (EとD) のパラメータを更新するか否か\n",
    "    :return loss_vae: tensor, VAEの損失, size=()\n",
    "    :return nll: tensor, 負の対数尤度, size=()\n",
    "    :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_encoder = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    input_encoder_lens = x_lens - 1\n",
    "    z, mean, lvar, loss_kl = E.forward(input_encoder, input_encoder_lens)\n",
    "    \n",
    "    if use_c_prior:\n",
    "        c = sample_c_prior(x.size(0))\n",
    "    else:\n",
    "        c = F.softmax(D.forward(input_encoder), dim=-1)\n",
    "    \n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    # Generator\n",
    "    input_generator = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    target_generator = x[:, 1:].contiguous() # [x_1, x_2, ..., </S>]\n",
    "    output_generator = G.forward(input_generator, zc)\n",
    "    \n",
    "    # 損失\n",
    "    nll_all = F.cross_entropy(output_generator.view(-1, vocab_size), target_generator.view(-1), ignore_index=PAD, size_average=False, reduce=False)\n",
    "    nll_mb = torch.sum(nll_all.view(output_generator.size(0), output_generator.size(1)), dim=1)\n",
    "    nll = nll_mb.mean()\n",
    "    \n",
    "    loss_vae = nll + lmd * loss_kl\n",
    "    \n",
    "    if is_train:\n",
    "        E.zero_grad()\n",
    "        G.zero_grad()\n",
    "        loss_vae.backward()\n",
    "        optimizer_E.step()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    return loss_vae, nll, loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2. $\\mathcal{L}_s(\\theta_D)$\n",
    "\n",
    "教師ありデータ ($x_L, c_L$) に対するDiscriminatorの識別損失を表します.\n",
    "\n",
    "第2段階の学習で使用します.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_s(\\theta_D) = -\\mathbb{E}_{\\mathcal{X}_L}\\left[\\log q_D(c_L|x_L)\\right]\n",
    "$$\n",
    "\n",
    "<img src=\"figure/loss_s.png\" width=\"800mm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_s(x, c):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param c: tensor, 潜在コードcのバッチ, size=(バッチサイズ, latent_c_size)\n",
    "    :return loss_s: tensor, size=()\n",
    "    \"\"\"\n",
    "    input_encoder = x[:, 1:-1] # [x_1, x_2, ..., x_T]\n",
    "    c_pred = D.forward(input_encoder) # (バッチサイズ, latent_c_size)\n",
    "    \n",
    "    loss_s = F.cross_entropy(c_pred, c)\n",
    "    \n",
    "    return loss_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3. $\\mathcal{L}_u(\\theta_D)$\n",
    "\n",
    "Generatorが生成した文に対するDiscriminatorの識別損失を表します.\n",
    "\n",
    "第2段階の学習で使用します.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_u(\\theta_D) = -\\mathbb{E}_{p_G(\\hat{x}|z,c)p(z)p(c)}\\left[\\log q_D(c|\\hat{x}) + \\beta\\mathcal{H}\\left(q_D(c'|\\hat{x})\\right)\\right]\n",
    "$$\n",
    "\n",
    "<img src=\"figure/loss_u.png\" width=\"800mm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_u():\n",
    "    \"\"\"\n",
    "    :return loss_u: tensor, size=()\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    x_hat = G.sample(zc, max_length, soft_decoding=False, greedy_decoding=False) # (バッチサイズ, 系列長)\n",
    "    \n",
    "    c_pred = D.forward(x_hat) # (バッチサイズ, latent_c_size)\n",
    "    \n",
    "    loss_u = F.cross_entropy(c_pred, c.argmax(dim=1)) - beta * F.log_softmax(c_pred, dim=-1).mean()\n",
    "    \n",
    "    return loss_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4. $\\mathcal{L}_{Attr, c}(\\theta_G)$\n",
    "\n",
    "事前分布からサンプリングされた属性 $c \\sim p(c)$ に対し, Generatorがどれだけそれに沿った文を生成できたかを表します.\n",
    "\n",
    "第2段階の学習で使用します.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_{Attr, c}(\\theta_G) = -\\mathbb{E}_{p(z)p(c)}\\left[\\log q_D\\left(c|\\tilde{G}_{\\tau(z,c)}\\right)\\right]\n",
    "$$\n",
    "\n",
    "<img src=\"figure/loss_attr_c.png\" width=\"800mm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.5. $\\mathcal{L}_{Attr, z}(\\theta_G)$\n",
    "\n",
    "サンプリングされた潜在変数 $z \\sim p(z)$に対し, Generatorがどれだけそれに沿って文を生成できたかを表します.\n",
    "\n",
    "第2段階の学習で使用します.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_{Attr, z}(\\theta_G) = \\mathbb{E}_{p(z)p(c)}\\left[\\mathrm{MSE}(z, q_E\\left(z|\\tilde{G}_{\\tau}(z,c)\\right))\\right]\n",
    "$$\n",
    "\n",
    "<img src=\"figure/loss_attr_z.png\" width=\"800mm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_attr():\n",
    "    \"\"\"\n",
    "    :return loss_attr_c: tensor, size=()\n",
    "    :return loss_attr_z: tensor, size=()\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "\n",
    "    # Generator\n",
    "    x_tilde = G.sample(zc, max_length, soft_decoding=True, greedy_decoding=False)\n",
    "\n",
    "    # Discriminator\n",
    "    c_pred = D.forward(x_tilde)\n",
    "    loss_attr_c = F.cross_entropy(c_pred, torch.argmax(c, dim=1))\n",
    "\n",
    "    # Encoder\n",
    "    z_pred, _, _, _ = E.forward(x_tilde)\n",
    "    loss_attr_z = F.mse_loss(z_pred, z)\n",
    "    \n",
    "    return loss_attr_c, loss_attr_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. VAE (Encoder + Generator) の学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1段階では属性のついていないテキストデータに対して通常のVAE (Encoder + Generator) の学習を行います. この段階では属性ラベル$c$は使用せず, 事前分布$p(c)$からサンプリングしたものを使用します.\n",
    "\n",
    "iLect環境で1epochあたり2分強かかります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_weight(step):\n",
    "    \"\"\"step数でアニーリングしたKL項の重みを取得する (0 -> 1)\n",
    "    :param step: int, 累積学習ステップ数\n",
    "    :return : float\n",
    "    \"\"\"\n",
    "    return (math.tanh((step - 3500)/1000) + 1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    E.train()\n",
    "    G.train()\n",
    "    nll_train = []\n",
    "    kl_train = []\n",
    "    for batch_x, _, batch_x_lens in dataloader_train:\n",
    "        lmd = get_kl_weight(step)\n",
    "        \n",
    "        loss, nll, loss_kl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=True)\n",
    "        nll_train.append(nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "#         break\n",
    "\n",
    "    # Valid\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    nll_valid = []\n",
    "    kl_valid = []\n",
    "    for batch_x, _, batch_x_lens in dataloader_valid:\n",
    "        loss, nll, loss_kl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=False)\n",
    "        nll_valid.append(nll.item())\n",
    "        kl_valid.append(loss_kl.item())\n",
    "        \n",
    "#         break\n",
    "    \n",
    "    print('EPOCH: {}, LMD: {:.2f}, Train [NLL: {:.2f}, KL: {:.2f}], Valid [NLL: {:.2f}, KL: {:.2f}], Elapsed Time: {:.2f}[s]'.format(\n",
    "        epoch + 1,\n",
    "        lmd,\n",
    "        np.mean(nll_train),\n",
    "        np.mean(kl_train),\n",
    "        np.mean(nll_valid),\n",
    "        np.mean(kl_valid),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたGeneratorを用いて文を生成してみます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(batch_size, max_length, c=None):\n",
    "    \"\"\"Generatorから文を生成する\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :param max_length: int, 生成文の最大長\n",
    "    :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "    :return : tensor, 生成文の属性, size=(バッチサイズ)\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    if c is None:\n",
    "        c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    x_hat = G.sample(zc, max_length, soft_decoding=False, greedy_decoding=True)\n",
    "    \n",
    "    return x_hat, c.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "E.eval()\n",
    "G.eval()\n",
    "\n",
    "x, _ = sample(n_samples, max_length)\n",
    "x = x.cpu().numpy()\n",
    "\n",
    "for i, x_i in enumerate(x):\n",
    "    x_i = ' '.join([vocab.id2word[i] for i in x_i])\n",
    "    x_i = re.sub(r' {}.*'.format(EOS_TOKEN), '', x_i)\n",
    "    print('{}. {}'.format(i, x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Encoder, Generator, Disciriminatorそれぞれの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2段階で属性付きのテキストデータを利用し, モデルが属性$c$に沿った文を生成できるようにEncoder, Generator, Discriminatorをそれぞれ学習させていきます.\n",
    "\n",
    "上で定義した損失関数を組み合わせ, それぞれの最終的な損失関数を定義していきます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminatorの損失関数\n",
    "教師ありデータ$(x_L, c_L)$に対する損失関数と, Generatorが潜在コード$c$の下で生成した文$\\tilde{x}$に対する損失関数を組み合わせます.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_D(\\theta_D) = \\mathcal{L}_s + \\lambda_u\\mathcal{L}_u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_D(x, c, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param c: tensor, 潜在コードcのバッチ, size=(バッチサイズ, latent_c_size)\n",
    "    :param is_train: bool, モデル (D) のパラメータを更新するか否か\n",
    "    :return loss_D: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_s = compute_loss_s(x, c)\n",
    "    loss_u = compute_loss_u()\n",
    "    \n",
    "    loss_D = loss_s + lmd_u * loss_u\n",
    "    \n",
    "    if is_train:\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generatorの損失関数\n",
    "VAEの損失関数, 潜在コード$c$に対する損失関数, 潜在変数$z$に対する損失関数を組み合わせます.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_G(\\theta_G) = \\mathcal{L}_{VAE} + \\lambda_c\\mathcal{L}_{Attr,c} + \\lambda_z\\mathcal{L}_{Attr, z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_G(x, x_lens, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_lens: tensor, 系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param is_train: bool, モデル (G) のパラメータを更新するか否か\n",
    "    :return loss_G: tensor, size=()\n",
    "    :return loss_vae: tensor, size=()\n",
    "    :return loss_attr_c: tensor, size=()\n",
    "    :return loss_attr_z: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_vae, _, _ = compute_loss_vae(x, x_lens, lmd=1, use_c_prior=False, is_train=False)\n",
    "    loss_attr_c, loss_attr_z = compute_loss_attr()\n",
    "\n",
    "    loss_G = loss_vae + lmd_c * loss_attr_c + lmd_z * loss_attr_z\n",
    "    \n",
    "    if is_train:\n",
    "        G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    return loss_G, loss_vae, loss_attr_c, loss_attr_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoderの損失関数\n",
    "VAEの損失関数をそのまま使用します.\n",
    "この際Encoderのパラメータに対してのみ学習を行い, Generatorのパラメータに対しては行いません.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_{E}(\\theta_E) = \\mathcal{L}_{VAE}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_E(x, x_lens, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_lens: tensor, 系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param is_train: bool, モデル (E) のパラメータを更新するか否か\n",
    "    :return loss_E: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_vae, _, _ = compute_loss_vae(x, x_lens, lmd=1, use_c_prior=False, is_train=False)\n",
    "    \n",
    "    loss_E = loss_vae\n",
    "    \n",
    "    if is_train:\n",
    "        E.zero_grad()\n",
    "        loss_E.backward()\n",
    "        optimizer_E.step()\n",
    "    \n",
    "    return loss_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習\n",
    "\n",
    "1epochあたり15分ほどかかります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    E.train()\n",
    "    G.train()\n",
    "    D.train()\n",
    "    loss_D_train = []\n",
    "    loss_G_train = []\n",
    "    loss_E_train = []\n",
    "    for batch_x, batch_c, batch_x_lens in dataloader_train:\n",
    "        # Discriminatorの学習\n",
    "        loss_D = compute_loss_D(batch_x, batch_c, is_train=True)\n",
    "        loss_D_train.append(loss_D.item())\n",
    "        \n",
    "        # Generatorの学習\n",
    "        loss_G, _, _, _ = compute_loss_G(batch_x, batch_x_lens, is_train=True)\n",
    "        loss_G_train.append(loss_G.item())\n",
    "        \n",
    "        # Encoderの学習\n",
    "        loss_E = compute_loss_E(batch_x, batch_x_lens, is_train=True)\n",
    "        loss_E_train.append(loss_E.item())\n",
    "        \n",
    "#         break\n",
    "    \n",
    "    # Valid\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    loss_D_valid = []\n",
    "    loss_G_valid = []\n",
    "    loss_E_valid = []\n",
    "    \n",
    "    pred_real_valid = []\n",
    "    gold_real_valid = []\n",
    "    pred_gene_valid = []\n",
    "    gold_gene_valid = []\n",
    "    for batch_x, batch_c, batch_x_lens in dataloader_valid:\n",
    "        # Discriminatorの検証\n",
    "        loss_D = compute_loss_D(batch_x, batch_c, is_train=False)\n",
    "        loss_D_valid.append(loss_D.item())\n",
    "        \n",
    "        c_pred = D.forward(batch_x).argmax(dim=1).tolist()\n",
    "        c_gold = batch_c.tolist()\n",
    "        pred_real_valid.extend(c_pred)\n",
    "        gold_real_valid.extend(c_gold)\n",
    "        \n",
    "        x_hat, c_gold = sample(batch_size, max_length)\n",
    "        c_pred = D.forward(x_hat).argmax(dim=1).tolist()\n",
    "        c_gold = c_gold.tolist()\n",
    "        pred_gene_valid.extend(c_pred)\n",
    "        gold_gene_valid.extend(c_gold)\n",
    "        \n",
    "        # Generatorの検証\n",
    "        loss_G, _, _, _ = compute_loss_G(batch_x, batch_x_lens, is_train=False)\n",
    "        loss_G_valid.append(loss_G.item())\n",
    "        \n",
    "        # Encoderの検証\n",
    "        loss_E = compute_loss_E(batch_x, batch_x_lens)\n",
    "        loss_E_valid.append(loss_E.item())\n",
    "        \n",
    "#         break\n",
    "    \n",
    "    print('''\n",
    "    EPOCH: {}, Elapsed Time: {:.2f}[s]\n",
    "    Train [D\\'s Loss: {:.2f}, G\\'s Loss: {:.2f}, E\\'s Loss: {:.2f}]\n",
    "    Valid [D\\'s Loss: {:.2f}, G\\'s Loss: {:.2f}, E\\'s Loss: {:.2f}, Accuracy for real seq: {:.2f}, Accuracy for generated seq: {:.2f}]\n",
    "    '''.format(\n",
    "        epoch + 1,\n",
    "        time.time() - start_time,\n",
    "        np.mean(loss_D_train),\n",
    "        np.mean(loss_G_train),\n",
    "        np.mean(loss_E_train),\n",
    "        np.mean(loss_D_valid),\n",
    "        np.mean(loss_G_valid),\n",
    "        np.mean(loss_E_valid),\n",
    "        accuracy_score(gold_real_valid, pred_real_valid),\n",
    "        accuracy_score(gold_gene_valid, pred_gene_valid),\n",
    "    ))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 指定した属性($c$)で生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前分布から $z, c$ をサンプリングしたときに, その属性$c$に沿った文が生成できているか確認してみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "E.eval()\n",
    "G.eval()\n",
    "\n",
    "c_neg = torch.eye(2)[torch.zeros(n_samples // 2, dtype=torch.long)].to(device)\n",
    "c_pos = torch.eye(2)[torch.ones(n_samples // 2, dtype=torch.long)].to(device)\n",
    "\n",
    "c = torch.cat([c_neg, c_pos], dim=0)\n",
    "\n",
    "x, c = sample(n_samples, max_length, c)\n",
    "x = x.cpu().numpy()\n",
    "\n",
    "for x_i, c_i in zip(x, c):\n",
    "    x_i = ' '.join([vocab.id2word[j] for j in x_i])\n",
    "    x_i = re.sub(r' {}.*'.format(EOS_TOKEN), '', x_i)\n",
    "    print('属性: {}\\n生成文: {}\\n'.format(c_i, x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考資料\n",
    "\n",
    "- 原論文: http://proceedings.mlr.press/v70/hu17e.html\n",
    "- PyTorch実装 (非公式) : https://github.com/wiseodd/controlled-text-generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
