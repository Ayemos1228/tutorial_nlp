{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(34)\n",
    "torch.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 3. WGAN-GP character言語モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_C_ITERS = 10\n",
    "N_ITERS = 10000\n",
    "BATCH_SIZE = 64\n",
    "VALID_INTERVAL = 100\n",
    "\n",
    "X_LEN = 32 # 生成する文の長さ\n",
    "Z_DIM = 128 # Gに入力するノイズzの次元数\n",
    "H_DIM = 512 # GとDの隠れ層の次元数\n",
    "\n",
    "K_SIZE = 5 # GとDの畳み込みのカーネルの幅\n",
    "\n",
    "LMD = 10 # 勾配ペナルティの係数\n",
    "\n",
    "TRAIN_X_PATH = 'data/billion_word/train_x.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Billion word コーパス (http://www.statmt.org/lm-benchmark/) を使用します.\n",
    "\n",
    "今回は単語レベルではなく文字レベルでの学習となるため, 新しくVocabを作ります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, chars):\n",
    "        self.char2id = {k: v for v, k in enumerate(chars)}\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        return [self.char2id[char] for char in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, x_len=32, n_data=10e+10):\n",
    "    data = []\n",
    "    for i, line in enumerate(open(path, encoding='utf-8')):\n",
    "        if i >= n_data:\n",
    "            break\n",
    "        \n",
    "        sentence = line.strip()\n",
    "        if len(sentence) < x_len:\n",
    "            continue\n",
    "\n",
    "        data.append(sentence[:x_len])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, data_x, device, batch_size, char_size):\n",
    "        self.data_x = data_x\n",
    "        self.pointer = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.char_size = char_size\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data_x = shuffle(self.data_x)\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pointer >= len(self.data_x):\n",
    "            self.reset()\n",
    "        \n",
    "        start = self.pointer\n",
    "        end = self.pointer + self.batch_size\n",
    "        self.pointer = end\n",
    "        \n",
    "        batch_x = [datum for datum in self.data_x[start:end]]\n",
    "        batch_x = self.convert_to_tensor(batch_x, torch.long)\n",
    "        batch_x = self.to_one_hot(batch_x)\n",
    "        \n",
    "        return batch_x\n",
    "    \n",
    "    def convert_to_tensor(self, batch, dtype):\n",
    "        return torch.tensor(batch, dtype=dtype).to(self.device)\n",
    "    \n",
    "    def to_one_hot(self, batch):\n",
    "        zeros = torch.zeros(batch.size(1), self.char_size).to(device) # (x_len, char_size)\n",
    "        return torch.stack([zeros.scatter(1, batch_n.unsqueeze(1), 1.0) for batch_n in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}')\n",
    "CHAR_SIZE = len(CHARS)\n",
    "\n",
    "vocab = Vocab(CHARS)\n",
    "\n",
    "sens_train_x = load_data(TRAIN_X_PATH, X_LEN)\n",
    "train_x = [vocab.encode(sen) for sen in sens_train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = Dataloader(train_x, device, BATCH_SIZE, CHAR_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator, DiscriminatorともにCNN + Residual Blockで実装します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_prior(batch_size):\n",
    "    \"\"\"事前分布p(z)からzをサンプリングする\n",
    "    :param batch_size: int.\n",
    "    \"\"\"\n",
    "    z = torch.randn(batch_size, Z_DIM).to(device)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, h_dim, k_size):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.Conv1 = nn.Conv1d(h_dim, h_dim, k_size, padding=int((k_size-1)/2))\n",
    "        self.Conv2 = nn.Conv1d(h_dim, h_dim, k_size, padding=int((k_size-1)/2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tensor (バッチサイズ, 語彙数, 系列長)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.relu(x)\n",
    "        x = self.Conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Conv2(x)\n",
    "        return residual + 0.3 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, x_len, char_size, k_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.Linear_1   = nn.Linear(z_dim, x_len * h_dim)\n",
    "        self.ResBlock_1 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_2 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_3 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_4 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_5 = ResBlock(h_dim, k_size)\n",
    "        self.Conv       = nn.Conv1d(h_dim, char_size, 1)\n",
    "        \n",
    "        self.h_dim = h_dim\n",
    "        self.x_len = x_len\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        :param z: tensor (バッチサイズ, z_dim)\n",
    "        \"\"\"\n",
    "        x = self.Linear_1(z).reshape(z.size(0), self.h_dim, self.x_len) # (バッチサイズ, h_dim, 系列長)\n",
    "        x = self.ResBlock_1(x)\n",
    "        x = self.ResBlock_2(x)\n",
    "        x = self.ResBlock_3(x)\n",
    "        x = self.ResBlock_4(x)\n",
    "        x = self.ResBlock_5(x)\n",
    "        x = self.Conv(x).permute(0, 2, 1)\n",
    "        x = F.softmax(x, dim=-1) # (バッチサイズ, 系列長, 語彙数)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, h_dim, x_len, char_size, k_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.Conv       = nn.Conv1d(char_size, h_dim, 1)\n",
    "        self.ResBlock_1 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_2 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_3 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_4 = ResBlock(h_dim, k_size)\n",
    "        self.ResBlock_5 = ResBlock(h_dim, k_size)\n",
    "        self.Linear     = nn.Linear(x_len * h_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tensor (バッチサイズ, 系列長, 語彙数)\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1) # (バッチサイズ, 語彙数, 系列長)\n",
    "        x = self.Conv(x)\n",
    "        x = self.ResBlock_1(x)\n",
    "        x = self.ResBlock_2(x)\n",
    "        x = self.ResBlock_3(x)\n",
    "        x = self.ResBlock_4(x)\n",
    "        x = self.ResBlock_5(x).reshape(x.size(0), x.size(1) * x.size(2))\n",
    "        x = self.Linear(x).squeeze()\n",
    "        return x # (バッチサイズ,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(Z_DIM, H_DIM, X_LEN, CHAR_SIZE, K_SIZE).to(device)\n",
    "D = Discriminator(H_DIM, X_LEN, CHAR_SIZE, K_SIZE).to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WGAN-GPではDiscriminator (Critic) の勾配を安定させるため, 勾配に対するペナルティ項を追加します.\n",
    "\n",
    "ペナルティ項の勾配の計算には真のデータと生成されたデータの内分点を用います.\n",
    "\n",
    "Dの誤差関数\n",
    "$$\n",
    "    \\mathcal{L}(\\theta_D) = D(\\tilde{x}) - D(x) + \\lambda\\left(||\\nabla_{\\hat{x}}D(\\hat{x}) - 1||_2\\right)^2\n",
    "$$\n",
    "\n",
    "Gの誤差関数\n",
    "$$\n",
    "    \\mathcal{L}(\\theta_G) = - D\\left(G(z)\\right)\n",
    "$$\n",
    "\n",
    "参考: [DL輪読会] Improved Training of Wasserstein GANs (https://www.slideshare.net/DeepLearningJP2016/dlimproved-training-of-wasserstein-gans-81010174)\n",
    "\n",
    "元論文: I. Gulrajani et al. \"Improved Training of Wasserstein GANs\". NIPS. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_D(x_real, train=False):\n",
    "    \"\"\"\n",
    "    :param x_real: tensor (バッチサイズ, 系列長, 語彙数)\n",
    "    :param train: bool\n",
    "    \"\"\"\n",
    "    batch_size = x_real.size(0)\n",
    "\n",
    "    # 偽データを生成\n",
    "    z = sample_z_prior(batch_size)\n",
    "    x_fake = G.forward(z)\n",
    "\n",
    "    # ソフト偽データを生成\n",
    "    eps = torch.rand(batch_size, 1, 1).to(device)\n",
    "    x_soft = eps * x_real + (1 - eps) * x_fake\n",
    "\n",
    "    # 真データ・偽データ・ソフト偽データを識別\n",
    "    y_real = D.forward(x_real)\n",
    "    y_fake = D.forward(x_fake)\n",
    "    y_soft = D.forward(x_soft)\n",
    "\n",
    "    # ソフト偽データの勾配を計算\n",
    "    y_soft_grad = autograd.grad(\n",
    "        outputs=y_soft,\n",
    "        inputs=x_soft,\n",
    "        grad_outputs=torch.ones(y_soft.size()).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0] # (batch_size, sen_len, char_size)\n",
    "\n",
    "    # 誤差を計算\n",
    "    grad_pen = ((y_soft_grad.norm(2, dim=1) - 1)**2).mean()\n",
    "    loss_D = y_fake.mean() - y_real.mean() + LMD * grad_pen\n",
    "    \n",
    "    if train:\n",
    "        # パラメータを更新\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_G(train=False):\n",
    "    \"\"\"\n",
    "    :param train: bool\n",
    "    \"\"\"\n",
    "    # 偽データを生成\n",
    "    z = sample_z_prior(BATCH_SIZE)\n",
    "    x_fake = G.forward(z)\n",
    "    \n",
    "    # 偽データを識別\n",
    "    y_fake = D.forward(x_fake)\n",
    "    \n",
    "    # 誤差を計算\n",
    "    loss_G = - y_fake.mean()\n",
    "    \n",
    "    if train:\n",
    "        # パラメータを更新\n",
    "        G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    return loss_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/gulrajani_arxiv2017.png\" width=\"600mm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_ITERS):\n",
    "    # ディスクリミネータを学習\n",
    "    losses_D = []\n",
    "    for _ in range(N_C_ITERS):\n",
    "        # 真データを取得\n",
    "        x_real = next(dataloader_train)\n",
    "        \n",
    "        # 誤差を計算 & パラメータを更新\n",
    "        loss_D = compute_loss_D(x_real, train=True)\n",
    "        losses_D.append(loss_D.item())\n",
    "        print(loss_D.item())\n",
    "    \n",
    "    # ジェネレータを学習\n",
    "    # 誤差を計算 & パラメータを更新\n",
    "    loss_G = compute_loss_G(train=True)\n",
    "    print(loss_G.item())\n",
    "    \n",
    "    if (i + 1) % VALID_INTERVAL == 0:\n",
    "        print('ITERS: {}, D\\'s Loss: {:.3f}, G\\'s Loss: {:.3f}'.format(\n",
    "            i + 1, np.mean(losses_D), loss_G.item()\n",
    "        ))\n",
    "        # 偽データを生成\n",
    "        n_samples = 3\n",
    "        z = sample_z_prior(n_samples)\n",
    "        x_fake = G.forward(z).argmax(2).cpu().numpy()\n",
    "        for n, x_fake_n in enumerate(x_fake):\n",
    "            print(n, ':', ''.join(vocab.id2char[i] for i in x_fake_n))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたジェネレータで文を生成してみます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "z = sample_z_prior(n_samples)\n",
    "x_fake = G.forward(z).argmax(2).cpu().numpy()\n",
    "for n, x_fake_n in enumerate(x_fake):\n",
    "    print(n, ':', ''.join(vocab.id2char[i] for i in x_fake_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "\n",
    "- 原論文: http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans\n",
    "- TensorFlow (公式) : https://github.com/igul222/improved_wgan_training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
