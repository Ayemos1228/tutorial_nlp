{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題: 文章の属性を制御したまま文章を生成するモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 課題2で実装したモデルを用いて, どれだけ指定した属性に沿って文章が生成できるかを競います.\n",
    "- 演習で用いた映画レビューのデータセットを用い, 文章の属性を制御しながら生成をおこなえるモデルを構築してください.\n",
    "- 学習させたモデルで肯定的なレビュー・否定的なレビューをそれぞれ500件 (合計1000件) 生成し, 行区切りでファイルに保存したものを提出してください (`sample_submission.txt`を参考にしてください).\n",
    "    - 前半500行は肯定的なレビュー, 後半500行は否定的なレビューとしてください.\n",
    "- 生成文の属性の評価には, 学習データで事前に訓練させたCNNテキスト識別モデルに対す予測結果の精度 (F1スコア) を用います.\n",
    "    - CNNで使用する単語ID辞書は事前に`vocab.dump`として用意してあります (下コード参照). 必要であれば使用してください."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 sample_submission.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "VOCAB_PATH = './vocab.dump'\n",
    "\n",
    "# 学習用データ\n",
    "TRAIN_X_PATH = './data/styletransfer/train_x.txt'\n",
    "TRAIN_Y_PATH = './data/styletransfer/train_y.txt'\n",
    "\n",
    "# 検証用データ\n",
    "VALID_X_PATH = './data/styletransfer/valid_x.txt'\n",
    "VALID_Y_PATH = './data/styletransfer/valid_y.txt'\n",
    "\n",
    "vocab = pickle.load(open(VOCAB_PATH, 'rb')) # 演習で用いたvocabと同じ形式です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下のサンプルコードを参考にしてください."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab\n",
    "\n",
    "np.random.seed(34)\n",
    "torch.manual_seed(34)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "embedding_size = 300 # 単語の埋め込み次元数\n",
    "hidden_size = 300 # LSTMの隠れ層次元数\n",
    "latent_z_size = 32  # 潜在変数の次元数\n",
    "latent_c_size = 2   # 潜在コードの次元数\n",
    "latent_size = latent_z_size + latent_c_size\n",
    "n_filters = 100 # Discriminator (CNN) のフィルター数\n",
    "\n",
    "max_length = 11\n",
    "min_count = 1 # 出現数がMIN_COUNT未満の単語は<UNK>に置き換える\n",
    "\n",
    "word_drop_rate = 0.5\n",
    "\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "\n",
    "beta = 0.1\n",
    "lmd_c = 0.1\n",
    "lmd_u = 0.1\n",
    "lmd_z = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data(path, n_data=10e+10):\n",
    "    data = []\n",
    "    for i, line in enumerate(open(path, encoding='utf-8')):\n",
    "        words = line.strip().split()\n",
    "        data.append(words)\n",
    "        if i + 1 >= n_data:\n",
    "            break\n",
    "    return data\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_x, data_y, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param data_x: list, 文章 (単語IDのリスト) のリスト\n",
    "        :param data_y: list, 属性ラベルのリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data = list(zip(data_x, data_y))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.data = shuffle(self.data)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "        \n",
    "        # バッチを取得\n",
    "        batch_x, batch_c = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
    "        \n",
    "        # 系列長で降順にソート\n",
    "        batch = sorted(zip(batch_x, batch_c), key=lambda x: len(x[0]), reverse=True)\n",
    "        batch_x, batch_c = zip(*batch)\n",
    "        \n",
    "        # 系列長を取得\n",
    "        batch_x = [[BOS] + x + [EOS] for x in batch_x]\n",
    "        batch_x_lens = [len(x) for x in batch_x]\n",
    "        \n",
    "        # <S>, </S>を付与 + 短い系列にパディング\n",
    "        max_length = max(batch_x_lens)\n",
    "        batch_x = [x + [PAD] * (max_length - len(x)) for x in batch_x]\n",
    "\n",
    "        # tensorに変換\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.long, device=device)\n",
    "        batch_c = torch.tensor(batch_c, dtype=torch.long, device=device)\n",
    "        batch_x_lens = torch.tensor(batch_x_lens, dtype=torch.long, device=device)\n",
    "        \n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "        \n",
    "        return batch_x, batch_c, batch_x_lens\n",
    "\n",
    "vocab = Vocab({\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}, UNK_TOKEN)\n",
    "\n",
    "sens_train_X = load_data(TRAIN_X_PATH)\n",
    "sens_valid_X = load_data(VALID_X_PATH)\n",
    "\n",
    "vocab.build_vocab(sens_train_X, min_count)\n",
    "\n",
    "train_X = [vocab.sentence_to_ids(sen) for sen in sens_train_X]\n",
    "valid_X = [vocab.sentence_to_ids(sen) for sen in sens_valid_X]\n",
    "\n",
    "train_Y = np.loadtxt(TRAIN_Y_PATH)\n",
    "valid_Y = np.loadtxt(VALID_Y_PATH)\n",
    "\n",
    "vocab_size = len(vocab.word2id)\n",
    "print('語彙数:', vocab_size)\n",
    "print('学習用データ数:', len(train_X))\n",
    "print('検証用データ数:', len(valid_X))\n",
    "\n",
    "train_X_stage1, train_X_stage2, train_Y_stage1, train_Y_stage2 = train_test_split(train_X, train_Y, test_size=0.1)\n",
    "\n",
    "dataloader_train_stage1 = DataLoader(train_X_stage1, train_Y_stage1, batch_size)\n",
    "dataloader_train_stage2 = DataLoader(train_X_stage2, train_Y_stage2, batch_size)\n",
    "\n",
    "dataloader_valid = DataLoader(valid_X, valid_Y, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_z_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力単語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_z_size: int, 潜在変数zの次元数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_m  = nn.Linear(hidden_size, latent_z_size)\n",
    "        self.linear_v  = nn.Linear(hidden_size, latent_z_size)\n",
    "    \n",
    "    def forward(self, x, x_lens=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 単語id列のバッチ or 単語の確率分布列のバッチ, size=(バッチサイズ, 系列長) or (バッチサイズ, 系列長, 語彙数)\n",
    "        :param x_lens: tensor, 単語id列の長さのバッチ, size=(バッチサイズ, 系列長)\n",
    "        :return z: tensor, サンプリングした潜在変数, size=(バッチサイズ, latent_size)\n",
    "        :return mean: tensor, 潜在変数の平均, size=(バッチサイズ, latent_size)\n",
    "        :return lvar: tensor, 潜在変数のlog分散, size=(バッチサイズ, latent_size)\n",
    "        :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "        \"\"\"\n",
    "        if x.dim() == 2: # xが単語IDのtensorの場合\n",
    "            x = self.embedding(x)\n",
    "            if x_lens is not None:\n",
    "                x = pack_padded_sequence(x, lengths=x_lens, batch_first=True) # <PAD>の部分は無視してエンコードする\n",
    "        elif x.dim() == 3: # xが単語の確率分布のtensorの場合\n",
    "            x = torch.matmul(x, self.embedding.weight)\n",
    "\n",
    "        _, (h_T, _) = self.lstm(x) # LSTM\n",
    "        \n",
    "        mean = self.linear_m(h_T[0]) # 平均\n",
    "        lvar = self.linear_v(h_T[0]) # 分散のlog\n",
    "        std = torch.exp(0.5 * lvar) # 標準偏差\n",
    "        \n",
    "        eps = torch.randn(mean.size()).to(device) # 標準正規分布からサンプリング\n",
    "        z = mean + std * eps # 潜在変数\n",
    "        \n",
    "        loss_kl = - 0.5 * torch.mean(torch.sum(1 + lvar - mean**2 - lvar.exp(), dim=1)) # KLダイバージェンスの計算\n",
    "\n",
    "        return z, mean, lvar, loss_kl\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_size, word_drop_rate):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 入力単語の語彙数\n",
    "        :param embedding_size: int, 単語埋め込みの次元数\n",
    "        :param hidden_size: int, LSTMの隠れ層の次元数\n",
    "        :param latent_size: int, 潜在変数zの次元数\n",
    "        :param word_drop_rate: int, 入力単語をunkに置き換える確率\n",
    "        \"\"\"\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size+latent_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear_h = nn.Linear(latent_size, hidden_size)\n",
    "        self.linear_c = nn.Linear(latent_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_drop_rate = word_drop_rate\n",
    "\n",
    "    def forward(self, x, zc):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "        :param zc: tensor, 潜在変数と潜在コードのバッチ, size=(バッチサイズ, latent_z_size+latent_size)\n",
    "        :return y: tensor, Decoderの出力, size=(バッチサイズ, 系列長, 語彙数)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(zc).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(zc).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        \n",
    "        # 単語を一定確率pで<unk>に置き換える\n",
    "        if self.training:\n",
    "            prob = torch.full_like(x, self.word_drop_rate, dtype=torch.float32).to(device)\n",
    "            mask = torch.bernoulli(prob)\n",
    "            x = torch.where(mask == 1, torch.full_like(x, UNK).to(device), x)\n",
    "        \n",
    "        x = self.embedding(x) # 単語の埋め込み\n",
    "        x = torch.cat([x, zc.unsqueeze(1).repeat(1, x.shape[1], 1)], dim=2) # 潜在変数+コードzcを毎時刻単語のembeddingにconcatする\n",
    "        \n",
    "        h, (_, _) = self.lstm(x, (h_0, c_0)) # LSTM\n",
    "        \n",
    "        y = F.softmax(self.out(h), dim=-1) # Softmax size=(バッチサイズ, 系列長, 語彙数)\n",
    "        \n",
    "        return y # size=(バッチサイズ, 系列帳, 語彙数)\n",
    "\n",
    "    def sample(self, zc, max_length, soft_decoding, greedy_decoding):\n",
    "        \"\"\"\n",
    "        :param zc: tensor, 潜在変数と潜在コードのバッチ, size=(バッチサイズ, latent_size)\n",
    "        :param max_length: int, 生成文の最大長\n",
    "        :param soft_decoding: bool, 各時刻の出力を単語IDとするか単語の確率分布とするか\n",
    "        :param greedy_decoding: bool, 各時刻で確率の最も高い単語を取得するか確率分布からサンプリングするか\n",
    "        :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "        \"\"\"\n",
    "        # 初期状態\n",
    "        h_0 = self.linear_h(zc).unsqueeze(0) # 隠れ層の初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        c_0 = self.linear_c(zc).unsqueeze(0) # cellの初期状態, size=(1, バッチサイズ, 隠れ層の次元数)\n",
    "        \n",
    "        # 最初の単語は<S>\n",
    "        x_0 = torch.full((zc.size(0), 1), BOS, dtype=torch.long).to(device) # size=(バッチサイズ, 1)\n",
    "\n",
    "        if soft_decoding:\n",
    "            # 単語IDをone-hot化, size=(バッチサイズ, 語彙数)\n",
    "            x_0 = torch.zeros((x_0.size(0), self.vocab_size), device=device).scatter(1, x_0, 1.0)\n",
    "    \n",
    "        zc = zc.unsqueeze(1) # size=(バッチサイズ, 1, latent_size)\n",
    "        \n",
    "        x_tm1, h_tm1, c_tm1 = x_0, h_0, c_0\n",
    "\n",
    "        x_generated = [] # 生成文を格納するlist\n",
    "        \n",
    "        if not soft_decoding:\n",
    "            flag = np.zeros(x_0.size(0), dtype=bool) # 出力が終わったかどうかのFlag\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if soft_decoding: # 単語の確率分布から埋め込みベクトルを取得\n",
    "                x_t = torch.matmul(x_tm1, self.embedding.weight).unsqueeze(1) # size=(バッチサイズ, 1, embedding_size)\n",
    "            else: # 単語IDから埋め込みベクトルを取得\n",
    "                x_t = self.embedding(x_tm1) # size=(バッチサイズ, 1, embedding_size)\n",
    "            \n",
    "            x_t = torch.cat([x_t, zc], dim=2) # 潜在変数+潜在コードと単語埋め込みベクトルを連結, size=(バッチサイズ, 1, embedding_size+latent_size)\n",
    "            \n",
    "            _, (h_t, c_t) = self.lstm(x_t, (h_tm1, c_tm1)) # LSTM\n",
    "            \n",
    "            y_t = F.softmax(self.out(h_t[0]), dim=-1) # Softmax, size=(バッチサイズ, 語彙数)\n",
    "            \n",
    "            if not soft_decoding:\n",
    "                if greedy_decoding: # 確率の一番高い単語を取得する\n",
    "                    y_t = y_t.argmax(1).unsqueeze(1)\n",
    "                else: # 確率分布からサンプリングする\n",
    "                    y_t = torch.multinomial(y_t, 1) # (バッチサイズ, 1)\n",
    "            \n",
    "            x_generated.append(y_t)\n",
    "\n",
    "            # t -> t-1\n",
    "            x_tm1, h_tm1, c_tm1 = y_t, h_t, c_t\n",
    "\n",
    "            if not soft_decoding:\n",
    "                # </S>が出力されたらFlagを更新する\n",
    "                flag_t = (y_t.squeeze().cpu().numpy() == EOS)\n",
    "                flag = np.logical_or(flag, flag_t)\n",
    "\n",
    "                # Bすべての系列で</S>が出力されたら終了\n",
    "                if np.all(flag):\n",
    "                    break\n",
    "\n",
    "        # listからtensorに変換する\n",
    "        if soft_decoding:\n",
    "            x_generated = torch.stack(x_generated, dim=1)\n",
    "        else:\n",
    "            x_generated = torch.cat(x_generated, dim=1)\n",
    "\n",
    "        return x_generated # size=(バッチサイズ, 系列長, 語彙数) or (バッチサイズ, 系列長)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, n_filters, latent_c_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv_1 = nn.Conv1d(embedding_size, n_filters, kernel_size=3)\n",
    "        self.conv_2 = nn.Conv1d(embedding_size, n_filters, kernel_size=4)\n",
    "        self.conv_3 = nn.Conv1d(embedding_size, n_filters, kernel_size=5)\n",
    "        self.out = nn.Linear(n_filters*3, latent_c_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tensor, 単語id列のバッチ or 単語の確率分布列のバッチ, size=(バッチサイズ, 系列長) or (バッチサイズ, 系列長, 語彙数)\n",
    "        :return x: tensor, size=(バッチサイズ, latent_c_size)\n",
    "        \"\"\"\n",
    "        x_len = x.size(1)\n",
    "\n",
    "        if x.dim() == 2: # xが単語IDのtensorの場合\n",
    "            x = self.embedding(x)\n",
    "        elif x.dim() == 3: # xが単語の確率分布のtensorの場合\n",
    "            x = torch.matmul(x, self.embedding.weight)\n",
    "\n",
    "        x = x.permute(0, 2, 1) # conv用に次元を入れ替え, size=(バッチサイズ, embedding_size, 系列長)\n",
    "\n",
    "        x1 = torch.tanh(self.conv_1(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-2)\n",
    "        x2 = torch.tanh(self.conv_2(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-3)\n",
    "        x3 = torch.tanh(self.conv_3(x)) # フィルター1に対して畳み込み, size=(バッチサイズ, フィルター数, 系列長-4)\n",
    "\n",
    "        x1 = F.max_pool1d(x1, x_len-2) # x1に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "        x2 = F.max_pool1d(x2, x_len-3) # x2に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "        x3 = F.max_pool1d(x3, x_len-4) # x3に対してpooling, size=(バッチサイズ, フィルター数, 1)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=1)[:, :, 0] # size=(バッチサイズ, フィルター数*3)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x # (バッチサイズ, latent_c_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_z_size': latent_z_size\n",
    "}\n",
    "\n",
    "G_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'latent_size': latent_size,\n",
    "    'word_drop_rate': word_drop_rate\n",
    "}\n",
    "\n",
    "D_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'n_filters': n_filters,\n",
    "    'latent_c_size': latent_c_size\n",
    "}\n",
    "\n",
    "E = Encoder(**E_args).to(device)\n",
    "G = Generator(**G_args).to(device)\n",
    "D = Discriminator(**D_args).to(device)\n",
    "\n",
    "optimizer_E = optim.Adam(E.parameters())\n",
    "optimizer_G = optim.Adam(G.parameters())\n",
    "optimizer_D = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_prior(batch_size):\n",
    "    \"\"\"事前分布p(z)からzをサンプリングする\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :return z: tensor, サンプリングされたz, size=(バッチサイズ, latent_z_size)\n",
    "    \"\"\"\n",
    "    z = torch.randn(batch_size, latent_z_size, device=device)\n",
    "    return z\n",
    "\n",
    "def sample_c_prior(batch_size):\n",
    "    \"\"\"事前分布p(c)からcをサンプリングする\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :return c: tensor, サンプリングされたc, size=(バッチサイズ, latent_c_size)\n",
    "    \"\"\"\n",
    "    weights = torch.ones(latent_c_size, dtype=torch.float).to(device)\n",
    "    c = torch.multinomial(weights, batch_size, replacement=True)\n",
    "    c = torch.eye(latent_c_size)[c] # one_hot化\n",
    "    return c\n",
    "\n",
    "def compute_loss_vae(x, x_lens, lmd, use_c_prior=True, is_train=False):\n",
    "    \"\"\"VAEの損失関数 (負の対数尤度 + lmd * KLダイバージェンス) を計算する\n",
    "    :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_len: tensor, 入力単語の系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param lmd: int, KLダイバージェンスの大きさをコントロールする係数\n",
    "    :param use_c_prior: bool, 潜在コードcを事前分布p(c)からサンプリングする か Discriminatorからの出力を利用するか\n",
    "    :param is_train: bool, モデル (EとD) のパラメータを更新するか否か\n",
    "    :return loss_vae: tensor, VAEの損失, size=()\n",
    "    :return nll: tensor, 負の対数尤度, size=()\n",
    "    :return loss_kl: tensor, KLダイバージェンス, size=()\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_encoder = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    input_encoder_lens = x_lens - 1\n",
    "    z, mean, lvar, loss_kl = E.forward(input_encoder, input_encoder_lens)\n",
    "    \n",
    "    if use_c_prior:\n",
    "        c = sample_c_prior(x.size(0))\n",
    "    else:\n",
    "        c = F.softmax(D.forward(input_encoder), dim=-1)\n",
    "    \n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    # Generator\n",
    "    input_generator = x[:, :-1] # [<S>, x_1, ..., x_T]\n",
    "    target_generator = x[:, 1:].contiguous() # [x_1, x_2, ..., </S>]\n",
    "    output_generator = G.forward(input_generator, zc)\n",
    "    \n",
    "    # 損失\n",
    "    nll_all = F.cross_entropy(output_generator.view(-1, vocab_size), target_generator.view(-1), ignore_index=PAD, reduction='none')\n",
    "    nll_mb = torch.sum(nll_all.view(output_generator.size(0), output_generator.size(1)), dim=1)\n",
    "    nll = nll_mb.mean()\n",
    "    \n",
    "    loss_vae = nll + lmd * loss_kl\n",
    "    \n",
    "    if is_train:\n",
    "        E.zero_grad()\n",
    "        G.zero_grad()\n",
    "        loss_vae.backward()\n",
    "        optimizer_E.step()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    return loss_vae, nll, loss_kl\n",
    "\n",
    "def compute_loss_s(x, c):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 入力単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param c: tensor, 潜在コードcのバッチ, size=(バッチサイズ, latent_c_size)\n",
    "    :return loss_s: tensor, size=()\n",
    "    \"\"\"\n",
    "    input_encoder = x[:, 1:-1] # [x_1, x_2, ..., x_T]\n",
    "    c_pred = D.forward(input_encoder) # (バッチサイズ, latent_c_size)\n",
    "    \n",
    "    loss_s = F.cross_entropy(c_pred, c)\n",
    "    \n",
    "    return loss_s\n",
    "\n",
    "def compute_loss_u():\n",
    "    \"\"\"\n",
    "    :return loss_u: tensor, size=()\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    x_hat = G.sample(zc, max_length, soft_decoding=False, greedy_decoding=False) # (バッチサイズ, 系列長)\n",
    "    \n",
    "    c_pred = D.forward(x_hat) # (バッチサイズ, latent_c_size)\n",
    "    \n",
    "    loss_u = F.cross_entropy(c_pred, c.argmax(dim=1)) - beta * F.log_softmax(c_pred, dim=-1).mean()\n",
    "    \n",
    "    return loss_u\n",
    "\n",
    "def compute_loss_attr():\n",
    "    \"\"\"\n",
    "    :return loss_attr_c: tensor, size=()\n",
    "    :return loss_attr_z: tensor, size=()\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "\n",
    "    # Generator\n",
    "    x_tilde = G.sample(zc, max_length, soft_decoding=True, greedy_decoding=False)\n",
    "\n",
    "    # Discriminator\n",
    "    c_pred = D.forward(x_tilde)\n",
    "    loss_attr_c = F.cross_entropy(c_pred, torch.argmax(c, dim=1))\n",
    "\n",
    "    # Encoder\n",
    "    z_pred, _, _, _ = E.forward(x_tilde)\n",
    "    loss_attr_z = F.mse_loss(z_pred, z)\n",
    "    \n",
    "    return loss_attr_c, loss_attr_z\n",
    "\n",
    "def get_kl_weight(step):\n",
    "    \"\"\"step数でアニーリングしたKL項の重みを取得する (0 -> 1)\n",
    "    :param step: int, 累積学習ステップ数\n",
    "    :return : float\n",
    "    \"\"\"\n",
    "    return (math.tanh((step - 3500)/1000) + 1) /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    E.train()\n",
    "    G.train()\n",
    "    nll_train = []\n",
    "    kl_train = []\n",
    "    for batch_x, _, batch_x_lens in dataloader_train_stage1:\n",
    "        lmd = get_kl_weight(step)\n",
    "        \n",
    "        loss, nll, loss_kl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=True)\n",
    "        nll_train.append(nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        break\n",
    "\n",
    "    # Valid\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    nll_valid = []\n",
    "    kl_valid = []\n",
    "    for batch_x, _, batch_x_lens in dataloader_valid:\n",
    "        loss, nll, loss_kl = compute_loss_vae(batch_x, batch_x_lens, lmd, is_train=False)\n",
    "        nll_valid.append(nll.item())\n",
    "        kl_valid.append(loss_kl.item())\n",
    "        \n",
    "        break\n",
    "    \n",
    "    print('EPOCH: {}, LMD: {:.2f}, Train [NLL: {:.2f}, KL: {:.2f}], Valid [NLL: {:.2f}, KL: {:.2f}], Elapsed Time: {:.2f}[s]'.format(\n",
    "        epoch + 1,\n",
    "        lmd,\n",
    "        np.mean(nll_train),\n",
    "        np.mean(kl_train),\n",
    "        np.mean(nll_valid),\n",
    "        np.mean(kl_valid),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    \n",
    "    break\n",
    "\n",
    "def sample(batch_size, max_length, c=None):\n",
    "    \"\"\"Generatorから文を生成する\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :param max_length: int, 生成文の最大長\n",
    "    :return x_hat: tensor, 生成文, size=(バッチサイズ, 系列長)\n",
    "    :return : tensor, 生成文の属性, size=(バッチサイズ)\n",
    "    \"\"\"\n",
    "    z = sample_z_prior(batch_size)\n",
    "    if c is None:\n",
    "        c = sample_c_prior(batch_size)\n",
    "    zc = torch.cat([z, c], dim=1)\n",
    "    \n",
    "    x_hat = G.sample(zc, max_length, soft_decoding=False, greedy_decoding=True)\n",
    "    \n",
    "    return x_hat, c.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各componentの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_D(x, c, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param c: tensor, 潜在コードcのバッチ, size=(バッチサイズ, latent_c_size)\n",
    "    :param is_train: bool, モデル (D) のパラメータを更新するか否か\n",
    "    :return loss_D: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_s = compute_loss_s(x, c)\n",
    "    loss_u = compute_loss_u()\n",
    "    \n",
    "    loss_D = loss_s + lmd_u * loss_u\n",
    "    \n",
    "    if is_train:\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    return loss_D\n",
    "\n",
    "def compute_loss_G(x, x_lens, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_lens: tensor, 系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param is_train: bool, モデル (G) のパラメータを更新するか否か\n",
    "    :return loss_G: tensor, size=()\n",
    "    :return loss_vae: tensor, size=()\n",
    "    :return loss_attr_c: tensor, size=()\n",
    "    :return loss_attr_z: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_vae, _, _ = compute_loss_vae(x, x_lens, lmd=1, use_c_prior=False, is_train=False)\n",
    "    loss_attr_c, loss_attr_z = compute_loss_attr()\n",
    "\n",
    "    loss_G = loss_vae + lmd_c * loss_attr_c + lmd_z * loss_attr_z\n",
    "    \n",
    "    if is_train:\n",
    "        G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    return loss_G, loss_vae, loss_attr_c, loss_attr_z\n",
    "\n",
    "def compute_loss_E(x, x_lens, is_train=False):\n",
    "    \"\"\"\n",
    "    :param x: tensor, 単語id列のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param x_lens: tensor, 系列長のバッチ, size=(バッチサイズ, 系列長)\n",
    "    :param is_train: bool, モデル (E) のパラメータを更新するか否か\n",
    "    :return loss_E: tensor, size=()\n",
    "    \"\"\"\n",
    "    loss_vae, _, _ = compute_loss_vae(x, x_lens, lmd=1, use_c_prior=False, is_train=False)\n",
    "    \n",
    "    loss_E = loss_vae\n",
    "    \n",
    "    if is_train:\n",
    "        E.zero_grad()\n",
    "        loss_E.backward()\n",
    "        optimizer_E.step()\n",
    "    \n",
    "    return loss_E\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    E.train()\n",
    "    G.train()\n",
    "    D.train()\n",
    "    loss_D_train = []\n",
    "    loss_G_train = []\n",
    "    loss_E_train = []\n",
    "    for batch_x, batch_c, batch_x_lens in dataloader_train_stage2:\n",
    "        # Discriminatorの学習\n",
    "        loss_D = compute_loss_D(batch_x, batch_c, is_train=True)\n",
    "        loss_D_train.append(loss_D.item())\n",
    "        \n",
    "        # Generatorの学習\n",
    "        loss_G, _, _, _ = compute_loss_G(batch_x, batch_x_lens, is_train=True)\n",
    "        loss_G_train.append(loss_G.item())\n",
    "        \n",
    "        # Encoderの学習\n",
    "        loss_E = compute_loss_E(batch_x, batch_x_lens, is_train=True)\n",
    "        loss_E_train.append(loss_E.item())\n",
    "        \n",
    "        break\n",
    "    \n",
    "    # Valid\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    loss_D_valid = []\n",
    "    loss_G_valid = []\n",
    "    loss_E_valid = []\n",
    "    \n",
    "    pred_real_valid = []\n",
    "    gold_real_valid = []\n",
    "    pred_gene_valid = []\n",
    "    gold_gene_valid = []\n",
    "    for batch_x, batch_c, batch_x_lens in dataloader_valid:\n",
    "        # Discriminatorの検証\n",
    "        loss_D = compute_loss_D(batch_x, batch_c, is_train=False)\n",
    "        loss_D_valid.append(loss_D.item())\n",
    "        \n",
    "        c_pred = D.forward(batch_x).argmax(dim=1).tolist()\n",
    "        c_gold = batch_c.tolist()\n",
    "        pred_real_valid.extend(c_pred)\n",
    "        gold_real_valid.extend(c_gold)\n",
    "        \n",
    "        x_hat, c_gold = sample(batch_size, max_length)\n",
    "        c_pred = D.forward(x_hat).argmax(dim=1).tolist()\n",
    "        c_gold = c_gold.tolist()\n",
    "        pred_gene_valid.extend(c_pred)\n",
    "        gold_gene_valid.extend(c_gold)\n",
    "        \n",
    "        # Generatorの検証\n",
    "        loss_G, _, _, _ = compute_loss_G(batch_x, batch_x_lens, is_train=False)\n",
    "        loss_G_valid.append(loss_G.item())\n",
    "\n",
    "        # Encoderの検証\n",
    "        loss_E = compute_loss_E(batch_x, batch_x_lens)\n",
    "        loss_E_valid.append(loss_E.item())\n",
    "        \n",
    "        break\n",
    "    \n",
    "    print('''\n",
    "    EPOCH: {}, Elapsed Time: {:.2f}[s]\n",
    "    Train [D\\'s Loss: {:.2f}, G\\'s Loss: {:.2f}, E\\'s Loss: {:.2f}]\n",
    "    Valid [D\\'s Loss: {:.2f}, G\\'s Loss: {:.2f}, E\\'s Loss: {:.2f}, Accuracy for real seq: {:.2f}, Accuracy for generated seq: {:.2f}]\n",
    "    '''.format(\n",
    "        epoch + 1,\n",
    "        time.time() - start_time,\n",
    "        np.mean(loss_D_train),\n",
    "        np.mean(loss_G_train),\n",
    "        np.mean(loss_E_train),\n",
    "        np.mean(loss_D_valid),\n",
    "        np.mean(loss_G_valid),\n",
    "        np.mean(loss_E_valid),\n",
    "        accuracy_score(gold_real_valid, pred_real_valid),\n",
    "        accuracy_score(gold_gene_valid, pred_gene_valid),\n",
    "    ))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.eval()\n",
    "G.eval()\n",
    "\n",
    "generated_sentences = []\n",
    "c_pos = torch.eye(2)[torch.ones(500, dtype=torch.long)]\n",
    "c_neg = torch.eye(2)[torch.ones(500, dtype=torch.long)]\n",
    "\n",
    "c = torch.cat([c_pos, c_neg], dim=0)\n",
    "\n",
    "for c_n in c:\n",
    "    x_hat = sample(1, max_length, c_n.unsqueeze(0))[0]\n",
    "    x_hat = x_hat.cpu().numpy()[0]\n",
    "    x_hat = ' '.join([vocab.id2word[i] for i in x_hat])\n",
    "    \n",
    "    generated_sentences.append(x_hat + '\\n')\n",
    "\n",
    "with open('tmp.txt', 'w') as f:\n",
    "    f.writelines(generated_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
