{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題: 文章の属性を制御したまま文章を生成するモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 課題2で実装したモデルを用いて, どれだけ指定した属性に沿って文章が生成できるかを競います.\n",
    "- 演習で用いた映画レビューのデータセットを用い, 文章の属性を制御しながら生成をおこなえるモデルを構築してください.\n",
    "- 学習させたモデルで肯定的なレビュー・否定的なレビューをそれぞれ500件 (合計1000件) 生成し, 行区切りでファイルに保存したものを提出してください (`sample_submission.txt`を参考にしてください).\n",
    "    - 前半500行は肯定的なレビュー, 後半500行は否定的なレビューとしてください.\n",
    "- 生成文の属性の評価には, 学習データで事前に訓練させたCNNテキスト識別モデルに対す予測結果の精度 (F1スコア) を用います.\n",
    "    - CNNで使用する単語ID辞書は事前に`vocab.dump`として用意してあります (下コード参照). 必要であれば使用してください."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 sample_submission.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "VOCAB_PATH = './vocab.dump'\n",
    "\n",
    "# 学習用データ\n",
    "TRAIN_X_PATH = './data/styletransfer/train_x.txt'\n",
    "TRAIN_Y_PATH = './data/styletransfer/train_y.txt'\n",
    "\n",
    "# 検証用データ\n",
    "VALID_X_PATH = './data/styletransfer/valid_x.txt'\n",
    "VALID_Y_PATH = './data/styletransfer/valid_y.txt'\n",
    "\n",
    "vocab = pickle.load(open(VOCAB_PATH, 'rb')) # 演習で用いたvocabと同じ形式です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下のサンプルコードを参考にしてください."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError: # iLect環境\n",
    "    os.chdir('/root/userspace/chap5')\n",
    "    from utils import Vocab\n",
    "\n",
    "np.random.seed(34)\n",
    "torch.manual_seed(34)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "embedding_size = 300 # 単語の埋め込み次元数\n",
    "hidden_size = 300 # LSTMの隠れ層次元数\n",
    "latent_z_size = 32  # 潜在変数の次元数\n",
    "latent_c_size = 2   # 潜在コードの次元数\n",
    "latent_size = latent_z_size + latent_c_size\n",
    "n_filters = 100 # Discriminator (CNN) のフィルター数\n",
    "\n",
    "max_length = 11\n",
    "min_count = 1 # 出現数がMIN_COUNT未満の単語は<UNK>に置き換える\n",
    "\n",
    "word_drop_rate = 0.5\n",
    "\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "\n",
    "beta = 0.1\n",
    "lmd_c = 0.1\n",
    "lmd_u = 0.1\n",
    "lmd_z = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data(path, n_data=10e+10):\n",
    "    data = []\n",
    "    for i, line in enumerate(open(path, encoding='utf-8')):\n",
    "        words = line.strip().split()\n",
    "        data.append(words)\n",
    "        if i + 1 >= n_data:\n",
    "            break\n",
    "    return data\n",
    "\n",
    "class DataLoader:\n",
    "    # WRITE ME\n",
    "\n",
    "vocab = Vocab({\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}, UNK_TOKEN)\n",
    "\n",
    "sens_train_X = load_data(TRAIN_X_PATH)\n",
    "sens_valid_X = load_data(VALID_X_PATH)\n",
    "\n",
    "vocab.build_vocab(sens_train_X, min_count)\n",
    "\n",
    "train_X = [vocab.sentence_to_ids(sen) for sen in sens_train_X]\n",
    "valid_X = [vocab.sentence_to_ids(sen) for sen in sens_valid_X]\n",
    "\n",
    "train_Y = np.loadtxt(TRAIN_Y_PATH)\n",
    "valid_Y = np.loadtxt(VALID_Y_PATH)\n",
    "\n",
    "vocab_size = len(vocab.word2id)\n",
    "print('語彙数:', vocab_size)\n",
    "print('学習用データ数:', len(train_X))\n",
    "print('検証用データ数:', len(valid_X))\n",
    "\n",
    "train_X_stage1, train_X_stage2, train_Y_stage1, train_Y_stage2 = train_test_split(train_X, train_Y, test_size=0.1)\n",
    "\n",
    "dataloader_train_stage1 = DataLoader(train_X_stage1, train_Y_stage1, batch_size)\n",
    "dataloader_train_stage2 = DataLoader(train_X_stage2, train_Y_stage2, batch_size)\n",
    "\n",
    "dataloader_valid = DataLoader(valid_X, valid_Y, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # WRITE ME\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    # WRITE ME\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # WRITE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_args = {\n",
    "    # WRITE ME\n",
    "}\n",
    "\n",
    "G_args = {\n",
    "    # WRITE ME\n",
    "}\n",
    "\n",
    "D_args = {\n",
    "    # WRITE ME\n",
    "}\n",
    "\n",
    "E = Encoder(**E_args).to(device)\n",
    "G = Generator(**G_args).to(device)\n",
    "D = Discriminator(**D_args).to(device)\n",
    "\n",
    "optimizer_E = optim.Adam(E.parameters())\n",
    "optimizer_G = optim.Adam(G.parameters())\n",
    "optimizer_D = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_prior(batch_size):\n",
    "    # WRITE ME\n",
    "\n",
    "def sample_c_prior(batch_size):\n",
    "    # WRITE ME\n",
    "\n",
    "def compute_loss_vae(x, x_lens, lmd, use_c_prior=True, is_train=False):\n",
    "    # WRITE ME\n",
    "\n",
    "def compute_loss_s(x, c):\n",
    "    # WRITE ME\n",
    "\n",
    "def compute_loss_u():\n",
    "    # WRITE ME\n",
    "\n",
    "def compute_loss_attr():\n",
    "    # WRITE ME\n",
    "\n",
    "def get_kl_weight(step):\n",
    "    # WRITE ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # WRITE ME\n",
    "\n",
    "def sample(batch_size, max_length, c=None):\n",
    "    # WRITE ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各componentの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_D(x, c, is_train=False):\n",
    "    # WRITE ME\n",
    "    return loss_D\n",
    "\n",
    "def compute_loss_G(x, x_lens, is_train=False):\n",
    "    # WRITE ME\n",
    "\n",
    "def compute_loss_E(x, x_lens, is_train=False):\n",
    "    # WRITE ME\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # WRITE ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.eval()\n",
    "G.eval()\n",
    "\n",
    "generated_sentences = []\n",
    "c_pos = torch.eye(2)[torch.ones(500, dtype=torch.long)]\n",
    "c_neg = torch.eye(2)[torch.ones(500, dtype=torch.long)]\n",
    "\n",
    "c = torch.cat([c_pos, c_neg], dim=0)\n",
    "\n",
    "for c_n in c:\n",
    "    x_hat = sample(1, max_length, c_n.unsqueeze(0))[0]\n",
    "    x_hat = x_hat.cpu().numpy()[0]\n",
    "    x_hat = ' '.join([vocab.id2word[i] for i in x_hat])\n",
    "    \n",
    "    generated_sentences.append(x_hat + '\\n')\n",
    "\n",
    "with open('tmp.txt', 'w') as f:\n",
    "    f.writelines(generated_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
