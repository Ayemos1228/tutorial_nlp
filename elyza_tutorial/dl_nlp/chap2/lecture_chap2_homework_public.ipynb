{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回講義　宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題：\n",
    "* データセット（`text`）で単語の分散表現を学習して、与えられた単語ペア（`word_pairs`）の類似度スコアをcos類似度で計算してください。\n",
    "* 計算した類似度スコアをsubmission.csvに出力して、submission.csvを提出してください。\n",
    "* submission.csvのフォーマットはsample_submission.csvを参考にしてください。\n",
    "* 学習用データセットはtext8というもので、事前に前処理済みの英語のデータセットです。ピリオドもなく文同士がすべて連結した単語の羅列になっているので注意してください。\n",
    "* text8の全てを学習に使うと時間がかかりすぎるので、今回はその一部(100万単語)を訓練用データセットに使います。\n",
    "* Word Similarity Taskで評価します。人手で評価した類似度スコアと予測した類似度スコアの相関をPearsonの相関係数によって計算して評価します。\n",
    "* CBOWやSkipgramなど好きなアルゴリズムを使って構いません。意欲のある方はSubsamplingやHierarchical Softmax([元論文](https://arxiv.org/abs/1310.4546)参照)を実装してみたりsubwordを利用するなどその他の論文を参考にしてみてください。\n",
    "\n",
    "## 注意：\n",
    "* 辞書に含める単語の最低出現頻度は**3**としてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要であれば変更してください\n",
    "import os\n",
    "os.chdir('/root/userspace/chap2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データセット概要\n",
    "with open(\"./data/text8\", \"r\") as f:\n",
    "    text = f.readline()\n",
    "text = text.strip().split()\n",
    "print(\"総単語数:\", len(text))\n",
    "print(\"総単語語彙数:\", len(set(text)))\n",
    "print(\"データ:\", \" \".join(text[:100]))\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PAD = 0 # <PAD>のID\n",
    "UNK = 1 # <UNK>のID\n",
    "PAD_TOKEN = '<PAD>' # paddingに使います\n",
    "UNK_TOKEN = '<UNK>' # 辞書にない単語\n",
    "\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, word2id={}):\n",
    "        \"\"\"\n",
    "        word2id: 単語(str)をインデックス(int)に変換する辞書\n",
    "        id2word: インデックス(int)を単語(str)に変換する辞書\n",
    "        \"\"\"\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
    "\n",
    "    def build_vocab(self, sentences, min_count=3):\n",
    "        # 各単語の出現回数の辞書を作成する\n",
    "        word_counter = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "        # min_count回以上出現する単語のみ語彙に加える\n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "\n",
    "        # 語彙に含まれる単語の出現回数を保持する\n",
    "        self.raw_vocab = {w: word_counter[w] for w in self.word2id.keys() if w in word_counter}\n",
    "\n",
    "def load_data():\n",
    "    with open(\"./data/text8\") as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip().split()\n",
    "    return line\n",
    "\n",
    "text = load_data()\n",
    "text = text[:1000000]\n",
    "\n",
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab([text], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = []\n",
    "with open(\"./data/sample_submission.csv\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip().split(\",\")\n",
    "        word1 = line[0]\n",
    "        word2 = line[1]\n",
    "        word_pairs.append([word1, word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダー\n",
    "# WRITE ME\n",
    "\n",
    "# モデル\n",
    "# WRITE ME\n",
    "\n",
    "# 学習\n",
    "# WRITE ME\n",
    "\n",
    "# 学習済みモデルの埋め込み行列\n",
    "embedding_matrix = # WRITE ME ex) model.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "pred_scores = []\n",
    "for pair in word_pairs:\n",
    "    w1 = embedding_matrix[vocab.word2id[pair[0]]]\n",
    "    w2 = embedding_matrix[vocab.word2id[pair[1]]]\n",
    "    score = np.dot(w1, w2)/np.linalg.norm(w1, ord=2)/np.linalg.norm(w2, ord=2)\n",
    "    pred_scores.append(score)\n",
    "\n",
    "with open(\"./data/submission.csv\", \"w\") as fout:\n",
    "    for pair, score in zip(word_pairs, pred_scores):\n",
    "        fout.write(pair[0] + \",\" + pair[1] + \",\" + str(score) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
